<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Chapter 9: Unsupervised Learning and Pattern Discovery | The Professional's Introduction to Data Science with Python</title>
    <meta name="description" content="A comprehensive guide for career transitioners into Data Science with Python.">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.C-O_2SOJ.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.BxNbBqlB.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.C7nZcP7s.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CDjunVez.js">
    <link rel="modulepreload" href="/assets/chapters_chapter-9.md.BV5SxM8D.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>The Professional&#39;s Introduction to Data Science with Python</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/chapters/intro.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Tutorial</span><!--]--></a><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://abishpius.com" target="_blank" rel="noreferrer" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>About Author</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/abishpius" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/abishpius" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Introduction</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/intro.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Welcome</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>The Book</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-1.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>1. The Data Science Landscape</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-2.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>2. Python Essentials</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-3.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>3. Mastering Pandas</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-4.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>4. Data Cleaning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-5.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>5. EDA & Visualization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-6.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>6. Statistical Foundations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-7.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>7. Predictive Modeling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-8.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>8. Classification Algorithms</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-9.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>9. Pattern Discovery</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-10.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>10. The Capstone Project</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _chapters_chapter-9" data-v-39a288b8><div><h1 id="chapter-9-unsupervised-learning-and-pattern-discovery" tabindex="-1">Chapter 9: Unsupervised Learning and Pattern Discovery <a class="header-anchor" href="#chapter-9-unsupervised-learning-and-pattern-discovery" aria-label="Permalink to &quot;Chapter 9: Unsupervised Learning and Pattern Discovery&quot;">​</a></h1><p>K-Means Clustering: Grouping Similar Data Points So far in this book, every algorithm we have mastered—from Linear Regression to Decision Trees—has relied on a teacher. In data science terms, we call these Supervised Learning algorithms. We fed the model a dataset containing both the inputs (features) and the correct answers (labels), such as &quot;House Price,&quot; &quot;Did Churn,&quot; or &quot;Is Fraud.&quot; The model’s job was simply to learn the mapping between the two. But what happens when there is no teacher? What if you have a massive database of customer transactions, but no column that says &quot;High Value&quot; or &quot;At Risk&quot;? What if you have thousands of server logs but no label indicating &quot;Error&quot; or &quot;Normal&quot;? Welcome to Unsupervised Learning. In this domain, we don&#39;t predict a target variable. Instead, we ask the machine to explore the data and discover hidden structures, patterns, and groupings that we humans might miss. Our first stop in this new landscape is K-Means Clustering, one of the most popular and intuitive algorithms for finding order in chaos. The Intuition: Organizing the Unknown Imagine you have just been hired as a marketing manager for a retail chain. You are handed a spreadsheet containing data on 10,000 customers—specifically, their Annual Income and Spending Score (a metric of how often they buy). Your boss asks: &quot;How many distinct types of customers do we have?&quot; You can&#39;t run a Logistic Regression because you don&#39;t have a target variable to predict. You don&#39;t know if there are three types of customers or ten. You simply want to group similar customers together. A side-by-side comparison. The left panel shows a scatter plot of raw data points (Income vs Spending) all in the same color (gray), looking like a disorganized cloud. The right panel shows the same dots clustered into five distinct colors, revealing specific groups like &#39;Low Income/High Spend&#39; and &#39;High Income/High Spend&#39;.</p><p>A side-by-side comparison. The left panel shows a scatter plot of raw data points (Income vs Spending) all in the same color (gray), looking like a disorganized cloud. The right panel shows the same dots clustered into five distinct colors, revealing specific groups like &#39;Low Income/High Spend&#39; and &#39;High Income/High Spend&#39;. This is exactly what K-Means does. It looks at the distance between data points and attempts to group them into $K$ distinct clusters, where points in the same cluster are similar to each other, and points in different clusters are dissimilar. How the Algorithm Works The &quot;K&quot; in K-Means represents the number of clusters you want to find. The &quot;Means&quot; refers to the average position (the center) of the data points in that cluster. Here is the algorithm in plain English:</p><ol><li>Initialization: You choose $K$ (e.g., 3). The algorithm places 3 points randomly on your data plot. These are called Centroids. 2. Assignment: Every single data point looks at the 3 centroids and &quot;joins&quot; the team of the centroid closest to it. 3. Update: Once all points have joined a team, the cluster center (centroid) is recalculated. The centroid moves to the mathematical average position of all the points in its team. 4. Repeat: Because the centroids moved, some points might now be closer to a different centroid. Steps 2 and 3 are repeated until the centroids stop moving (convergence). A 4-step diagram showing the K-Means iteration. Step 1: Random centroids appear on a scatterplot. Step 2: Points are color-coded based on the nearest centroid. Step 3: Centroids move to the geometric center of their new color groups. Step 4: The final stable state where centroids no longer move.</li></ol><p>A 4-step diagram showing the K-Means iteration. Step 1: Random centroids appear on a scatterplot. Step 2: Points are color-coded based on the nearest centroid. Step 3: Centroids move to the geometric center of their new color groups. Step 4: The final stable state where centroids no longer move. Implementing K-Means in Python Let&#39;s apply this to our hypothetical customer dataset. We will use scikit-learn, the same library we used for regression and classification. First, we generate some synthetic data to represent our customers.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_blobs</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.cluster </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KMeans</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.preprocessing </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Generate synthetic customer data</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We create 300 samples with 2 features (Income, Spending Score)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># centers=4 implies there are naturally 4 groups in this data</span></span></code></pre></div><p>X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)</p><h1 id="convert-to-dataframe-for-easier-viewing" tabindex="-1">Convert to DataFrame for easier viewing <a class="header-anchor" href="#convert-to-dataframe-for-easier-viewing" aria-label="Permalink to &quot;Convert to DataFrame for easier viewing&quot;">​</a></h1><p>df = pd.DataFrame(X, columns=[&#39;Annual_Income&#39;, &#39;Spending_Score&#39;])</p><h1 id="_2-preprocessing-scaling-is-critical" tabindex="-1">2. Preprocessing: Scaling is Critical <a class="header-anchor" href="#_2-preprocessing-scaling-is-critical" aria-label="Permalink to &quot;2. Preprocessing: Scaling is Critical&quot;">​</a></h1><h1 id="k-means-calculates-distance-if-one-variable-is-in-millions-income" tabindex="-1">K-Means calculates distance. If one variable is in millions (Income) <a class="header-anchor" href="#k-means-calculates-distance-if-one-variable-is-in-millions-income" aria-label="Permalink to &quot;K-Means calculates distance. If one variable is in millions (Income)&quot;">​</a></h1><h1 id="and-another-in-single-digits-family-size-income-will-dominate" tabindex="-1">and another in single digits (Family Size), Income will dominate. <a class="header-anchor" href="#and-another-in-single-digits-family-size-income-will-dominate" aria-label="Permalink to &quot;and another in single digits (Family Size), Income will dominate.&quot;">​</a></h1><p>scaler = StandardScaler() X_scaled = scaler.fit_transform(df)</p><h1 id="_3-initialize-and-fit-k-means" tabindex="-1">3. Initialize and Fit K-Means <a class="header-anchor" href="#_3-initialize-and-fit-k-means" aria-label="Permalink to &quot;3. Initialize and Fit K-Means&quot;">​</a></h1><h1 id="let-s-assume-we-want-to-find-4-clusters" tabindex="-1">Let&#39;s assume we want to find 4 clusters <a class="header-anchor" href="#let-s-assume-we-want-to-find-4-clusters" aria-label="Permalink to &quot;Let&#39;s assume we want to find 4 clusters&quot;">​</a></h1><p>kmeans = KMeans(n_clusters=4, random_state=42) kmeans.fit(X_scaled)</p><h1 id="_4-get-the-cluster-labels" tabindex="-1">4. Get the Cluster Labels <a class="header-anchor" href="#_4-get-the-cluster-labels" aria-label="Permalink to &quot;4. Get the Cluster Labels&quot;">​</a></h1><h1 id="this-assigns-a-number-0-1-2-or-3-to-every-customer" tabindex="-1">This assigns a number (0, 1, 2, or 3) to every customer <a class="header-anchor" href="#this-assigns-a-number-0-1-2-or-3-to-every-customer" aria-label="Permalink to &quot;This assigns a number (0, 1, 2, or 3) to every customer&quot;">​</a></h1><p>df[&#39;Cluster_Label&#39;] = kmeans.labels_</p><p>print(df.head()) The output will look like a standard dataframe, but with a new column: Cluster_Label. This label is the pattern the algorithm discovered. You can now query your data: &quot;Show me the average income of Cluster 1 vs Cluster 2.&quot; The Million Dollar Question: How do we choose K? In the example above, I cheated. I told the computer to look for 4 clusters because I generated the data with 4 centers. In the real world, you won&#39;t know the answer. Should you segment your customers into 3 groups? 5 groups? 10? To solve this, we use a technique called the Elbow Method. We run the K-Means algorithm multiple times, increasing $K$ from 1 to 10. For each run, we calculate the Inertia (also known as Within-Cluster Sum of Squares). Inertia measures how tightly the data points are packed around their centroids.</p><ul><li>Lower Inertia is better (it means clusters are tight).</li><li>However, if $K$ equals the number of data points, inertia is 0 (perfect), but the clusters are meaningless. We look for the &quot;Elbow&quot;—the point where adding more clusters stops giving us significant gains in compactness.</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">inertia_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Test K from 1 to 10</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> k </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">11</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kmeans </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KMeans(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_clusters</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">k, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">random_state</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kmeans.fit(X_scaled)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    inertia_list.append(kmeans.inertia_)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Plotting the Elbow</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.plot(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">11</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), inertia_list, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">marker</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;o&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;The Elbow Method&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Number of clusters (K)&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Inertia&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A line graph representing the </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Elbow Method&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis represents </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Number of Clusters (k)&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the Y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis represents </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Inertia&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The line drops steeply </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> k</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> to k</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, then bends significantly at k</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> flattens out afterwards. The point at k</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> is</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> highlighted </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Elbow&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span></span></code></pre></div><p>A line graph representing the &#39;Elbow Method&#39;. The X-axis represents &#39;Number of Clusters (k)&#39; and the Y-axis represents &#39;Inertia&#39;. The line drops steeply from k=1 to k=2, then bends significantly at k=4, and flattens out afterwards. The point at k=4 is highlighted as the &#39;Elbow&#39;. In the graph above, you would see a sharp decline in inertia that flattens out after $K=4$. That &quot;elbow&quot; point suggests that 4 is the optimal balance between simplicity and accuracy. Business Application: Why use K-Means? For a career transitioner, understanding the application is just as important as the code. Here is where K-Means shines in industry:</p><ol><li>Customer Segmentation: As discussed, grouping customers by behavior to send targeted marketing campaigns (e.g., &quot;Budget Shoppers&quot; vs. &quot;Big Spenders&quot;). 2. Inventory Management: Clustering products based on sales velocity and seasonality to optimize warehouse placement. 3. Bot Detection: Clustering web traffic. Normal users usually fall into one large cluster; bots often engage in repetitive behaviors that form distinct, smaller clusters or outliers. 4. Document Classification: Grouping thousands of unlabelled support tickets into categories like &quot;Login Issues,&quot; &quot;Billing,&quot; or &quot;Feature Requests&quot; based on word usage. Limitations and Pitfalls While K-Means is a workhorse of unsupervised learning, it is not magic. Keep these limitations in mind:</li></ol><ul><li>Sensitivity to Outliers: One massive outlier can pull a centroid away from the main group, distorting the cluster. It is often wise to remove outliers before clustering.</li><li>Spherical Assumption: K-Means assumes clusters are round balls. If your data forms complex shapes (like a crescent moon or a ring), K-Means will fail to separate them correctly.</li><li>Scaling is Mandatory: As noted in the code, if you do not scale your data (using StandardScaler or MinMaxScaler), the feature with the largest numeric range will dictate the clusters. In the next section, we will look at Hierarchical Clustering, an alternative method that creates a &quot;family tree&quot; of data points, allowing us to visualize relationships without pre-selecting the number of clusters. Dimensionality Reduction: Simplifying Complex Datasets In the previous section on K-Means Clustering, we successfully grouped data points without labels. We took a dataset and asked the computer to &quot;find the structure.&quot; This works beautifully when you are dealing with two or three variables—for example, clustering customers based on Age and Spending Score. You can easily visualize this on a 2D plot (X and Y axis) and see the clusters separate. But real-world business data is rarely that simple. Imagine you are analyzing customer behavior for a massive e-commerce platform. You aren&#39;t just looking at Age and Spending. You have data on: Time on site Number of clicks Average cart value Geographic latitude/longitude Frequency of returns Days since last login * ...and 40 other columns. You now have a 50-dimensional dataset. Not only is this impossible for the human brain to visualize, but it also creates a computational problem known as the Curse of Dimensionality. As you add more features (dimensions), the data becomes &quot;sparse,&quot; meaning the data points are so far apart in that high-dimensional space that distance-based algorithms (like K-Means) struggle to determine what is close and what is far. To solve this, we need a way to reduce the number of columns without losing the critical information contained within them. We need Dimensionality Reduction. The Concept: Compression and Summarization Think of Dimensionality Reduction as an &quot;Executive Summary&quot; for your dataset. If you submit a 50-page report to a CEO, they might ask for a one-page summary. That summary doesn&#39;t just delete pages 2 through 50; it synthesizes the most important points from all 50 pages into a condensed format. In Data Science, we do this to: 1. Visualize data: We can squeeze 50 columns down to 2 or 3 so we can plot them on a chart. 2. Improve performance: Fewer columns mean faster processing for machine learning models. 3. Remove noise: It helps filter out irrelevant details, focusing the model on the signal. The most popular technique for this is Principal Component Analysis (PCA). Principal Component Analysis (PCA) PCA is a statistical procedure that converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called Principal Components. That sounds complex, so let’s use a visual analogy. Imagine you are holding a teapot. You want to take one photograph that best describes the shape of the teapot. Angle A (Top-down): You only see a circle (the lid). You lose the information about the handle and the spout. Angle B (Side view): You see the height, the spout, and the handle. This captures the most &quot;variance&quot; or information about the object. PCA effectively rotates the object (your data) in high-dimensional space to find the &quot;best angle&quot;—the angle that captures the most variance (information) and projects the data onto that angle. A diagram comparing two 2D projections of a 3D object (a teapot). The left projection is top-down, resulting in a simple circle (Low Information/Low Variance). The right projection is from the side, showing the spout and handle (High Information/High Variance). Arrows indicate that PCA selects the view with the highest variance.</li></ul><p>A diagram comparing two 2D projections of a 3D object (a teapot). The left projection is top-down, resulting in a simple circle (Low Information/Low Variance). The right projection is from the side, showing the spout and handle (High Information/High Variance). Arrows indicate that PCA selects the view with the highest variance. How PCA Works (The Non-Math Version)</p><ol><li>Standardization: First, PCA requires that all data be on the same scale. If one column is &quot;Salary&quot; (ranging from 30,000 to 150,000) and another is &quot;Age&quot; (ranging from 18 to 65), the Salary column will dominate simply because the numbers are bigger. We scale them so they compete fairly. 2. Finding the Axis of Variance: The algorithm looks for a line through the data where the data points are most spread out. This line becomes Principal Component 1 (PC1). It represents the strongest pattern in the dataset. 3. Finding the Second Axis: It then looks for a second line that is perpendicular (orthogonal) to the first one that captures the next most spread out direction. This is Principal Component 2 (PC2). 4. Repeat: This continues until we have as many components as we started with dimensions. However, the magic is that usually, the first few components (PC1 and PC2) capture 80-90% of the information. We can keep those two and discard the rest. Implementing PCA in Python Let&#39;s apply this to a dataset. We will use the famous &quot;Wine&quot; dataset available in Scikit-Learn. It contains chemical analysis of wines grown in Italy, with 13 different features (Alcohol, Malic acid, Ash, Magnesium, etc.). Our goal: Squash these 13 dimensions down to 2 so we can plot the wines on a scatter plot.</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seaborn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sns</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> load_wine</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.preprocessing </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.decomposition </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PCA</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Load the Data</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> load_wine()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(data.data, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data.feature_names)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Add the target (the type of wine: 0, 1, or 2) for coloring the plot later</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Wine_Type&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> data.target</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Original Dataset Shape: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df.shape</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output will be (178, 14) - 178 rows, 13 features + 1 target</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Standardize the Data (Crucial Step!)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We separate features (X) from the target (y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> data.feature_names</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df.loc[:, features].values</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df.loc[:, [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Wine_Type&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]].values</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Scale features to have mean=0 and variance=1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x_scaled </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler().fit_transform(x)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. Apply PCA</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We tell PCA we want to reduce down to 2 components</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PCA(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_components</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">principalComponents </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pca.fit_transform(x_scaled)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Create a new DataFrame with the two components</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca_df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">principalComponents, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PC1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PC2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Concatenate the target variable back for visualization</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">final_df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.concat([pca_df, df[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Wine_Type&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">axis</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;New Dataset Shape: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">final_df.shape</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output will be (178, 3) - 178 rows, 2 components + 1 target</span></span></code></pre></div><p>We have successfully transformed a spreadsheet with 13 columns of chemical data into a dataframe with just two abstract columns: PC1 and PC2. Interpreting the Results You might ask, &quot;What does the PC1 column represent? Is it Alcohol?&quot; The answer is no. PC1 is a mathematical mixture of all the original 13 features combined. It might be 30% Alcohol, 20% Magnesium, and -10% Malic Acid. It is an abstract feature that represents the dominant variance in the data. Now, let&#39;s visualize the result. Remember, we couldn&#39;t visualize 13 dimensions, but we can easily visualize 2.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 4. Visualize the 2D Projection</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sns.scatterplot(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PC1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PC2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    hue</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Wine_Type&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    palette</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;viridis&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">final_df, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    s</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PCA of Wine Dataset: 13 Dimensions reduced to 2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">fontsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">15</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Principal Component 1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">fontsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Principal Component 2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">fontsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A scatter plot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Principal Component 1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Principal Component 2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the Y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis. There are three distinct clusters of dots colored Purple, Green, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Yellow. The clusters are relatively well</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">separated, showing that the data reduction preserved the differences between the wine types.</span></span></code></pre></div><p>A scatter plot with &quot;Principal Component 1&quot; on the X-axis and &quot;Principal Component 2&quot; on the Y-axis. There are three distinct clusters of dots colored Purple, Green, and Yellow. The clusters are relatively well-separated, showing that the data reduction preserved the differences between the wine types. Even though we threw away 11 dimensions of data, we can see distinct clusters. This tells us that the different types of wine are chemically distinct, and PCA was able to capture those differences in just two dimensions. The &quot;Explained Variance&quot; Ratio How much information did we lose? PCA gives us a metric called Explained Variance Ratio. This tells us what percentage of the original dataset&#39;s information is held within our new components.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Explained Variance Ratio: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca.explained_variance_ratio_</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Total Information Retained: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(pca.explained_variance_ratio_) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 100</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">%&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Typical Output: Explained Variance Ratio: [0.3619, 0.1920] Total Information Retained: 55.41% In this example, PC1 holds 36% of the information, and PC2 holds 19%. Together, they hold about 55% of the original variance. While we lost 45% of the details, we kept enough signal to clearly distinguish the wine types in the plot above. When to Use Dimensionality Reduction As a data scientist, you will reach for PCA in these scenarios:</p><ol><li>Exploratory Data Analysis (EDA): When you get a new dataset with 100 columns and want to see if there are obvious groups or outliers, PCA allows you to plot the &quot;shape&quot; of the data immediately. 2. Addressing Overfitting: If you have too many features (columns) and not enough rows of data, supervised learning models (like Logistic Regression) can get confused. Reducing dimensions helps the model focus on the general patterns rather than memorizing noise. 3. Image Processing: Images are made of pixels. A small 28x28 pixel image has 784 dimensions (columns). PCA is excellent at compressing images by finding the &quot;principal components&quot; of the visual shapes (like curves and loops) rather than analyzing every single pixel. Summary Dimensionality Reduction is the art of simplification. By applying PCA, we traded specific details (like the exact magnesium level of a specific wine) for a broader understanding of the dataset&#39;s structure. We have now explored how to find patterns without labels using Clustering and how to simplify complex data using Dimensionality Reduction. In the next chapter, we will shift gears completely and discuss how to handle data that isn&#39;t numbers in a spreadsheet at all—we are moving into the world of Text Analysis and Natural Language Processing. Case Study: Customer Segmentation for Targeted Marketing We have now arrived at the intersection of the technical and the strategic. In the previous two sections, we laid the groundwork: K-Means gave us the algorithmic engine to group similar data points, and Dimensionality Reduction (PCA) gave us the ability to distill complex, multi-variable data into something manageable and visual. Now, we apply these unsupervised learning techniques to one of the most universal business challenges: Marketing Strategy. Unlike the Employee Attrition case study, where we had historical data telling us exactly who left the company (Supervised Learning), we are now entering the unknown. We don&#39;t have labels like &quot;Good Customer&quot; or &quot;Bad Customer.&quot; We simply have raw transaction data. Our goal is to let the algorithms discover the natural groupings within our customer base so we can move away from &quot;spray and pray&quot; marketing and toward data-driven personalization. The Business Problem: The &quot;One-Size-Fits-All&quot; Trap Imagine you are the Data Scientist for ShopRight, a mid-sized e-commerce retailer. The marketing director approaches you with a problem: &quot;We are sending the same 15% off coupon to everyone. We’re losing money giving discounts to loyal customers who would have bought anyway, and we aren&#39;t offering enough incentives to bring back customers who haven&#39;t shopped in months.&quot; To solve this, we will perform Customer Segmentation. Specifically, we will implement a technique known as RFM Analysis, turbo-charged by K-Means clustering. The Data Strategy: RFM Analysis RFM is a classic marketing framework that quantifies customer behavior using three dimensions: 1. Recency (R): How many days has it been since the customer&#39;s last purchase? (Lower is better). 2. Frequency (F): How many times has the customer purchased? (Higher is better). 3. Monetary Value (M): What is the total amount the customer has spent? (Higher is better). By clustering customers based on these three features, we can identify distinct personas automatically. Step 1: Data Preparation and Feature Engineering In a real-world scenario, your data would live in a SQL database full of transaction logs. For this case study, let&#39;s generate a synthetic dataset that mimics a retail environment.</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_blobs</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Setting a seed for reproducibility</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">np.random.seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Generating synthetic customer data (Recency, Frequency, Monetary)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We create 4 distinct &quot;centers&quot; of customer behavior to simulate reality</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data, true_labels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_blobs(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_samples</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">centers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cluster_std</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Creating a DataFrame</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(data, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Recency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Frequency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Monetary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Adjusting the data to look like real RFM values</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Recency: Days since last purchase (e.g., 1 to 365)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Recency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.abs(df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Recency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Frequency: Number of purchases (e.g., 1 to 50)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Frequency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.abs(df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Frequency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Monetary: Total spend (e.g., $50 to $5000)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Monetary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.abs(df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Monetary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 50</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(df.head())</span></span></code></pre></div><p>Why can&#39;t we just feed this into K-Means immediately? Look at the scale of the numbers. Monetary values might be in the thousands (e.g., $2,500), while Frequency might be single digits (e.g., 5 purchases). K-Means uses Euclidean distance (the ruler method) to calculate similarity. If we don&#39;t fix this, the algorithm will think a difference of $100 is vastly more important than a difference of 5 purchases, simply because the number is bigger. We must standardize the data so each feature contributes equally to the distance calculation.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.preprocessing </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">scaler </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df_scaled </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scaler.fit_transform(df)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Convert back to DataFrame for easier handling later</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df_scaled </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(df_scaled, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Recency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Frequency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Monetary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre></div><p>Step 2: Finding the Optimal Number of Segments How many customer segments do we have? 3? 5? 10? Because this is unsupervised learning, we don&#39;t know the &quot;right&quot; answer. We use the Elbow Method (introduced in the K-Means section) to find the sweet spot where we minimize the variance within clusters without over-complicating the model.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.cluster </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KMeans</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">inertia </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">range_val </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> range_val:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kmeans </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KMeans(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_clusters</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">i, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">random_state</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kmeans.fit(df_scaled)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    inertia.append(kmeans.inertia_)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.plot(range_val, inertia, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bx-&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Values of K&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Inertia&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;The Elbow Method using Inertia&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A line graph plotting </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Values of K&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> through </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) against </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Inertia&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the Y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis. The line drops steeply </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> K</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> to K</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> then flattens out significantly after K</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, creating a distinct </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;elbow&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> shape at K</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span></span></code></pre></div><p>A line graph plotting &quot;Values of K&quot; on the X-axis (1 through 10) against &quot;Inertia&quot; on the Y-axis. The line drops steeply from K=1 to K=3 and then flattens out significantly after K=4, creating a distinct &quot;elbow&quot; shape at K=4. Looking at the plot above, the &quot;elbow&quot; occurs around K=4. This suggests that dividing our customers into four groups gives us the best balance of cohesion and simplicity. Step 3: Building the Model and Visualizing Results Now we run the K-Means algorithm with 4 clusters.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Apply K-Means with 4 clusters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kmeans </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KMeans(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_clusters</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">random_state</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kmeans.fit(df_scaled)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Assign the cluster labels back to our ORIGINAL (non-scaled) dataframe</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kmeans.labels_</span></span></code></pre></div><p>We now have a &quot;Cluster&quot; column attached to every customer. But how do we visualize this? Our data is 3-dimensional (Recency, Frequency, Monetary), but our computer screens are 2-dimensional. This is where we apply the concept from the previous section: Dimensionality Reduction (PCA). We will squash the 3 dimensions down to 2 just for the sake of visualization.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.decomposition </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PCA</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seaborn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sns</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Reduce dimensions to 2 for plotting</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PCA(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_components</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca_components </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pca.fit_transform(df_scaled)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Create a temporary dataframe for the plot</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df_pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca_components, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PCA1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PCA2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df_pca[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Plotting</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sns.scatterplot(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PCA1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;PCA2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">hue</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df_pca, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">palette</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;viridis&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">s</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Customer Segments Visualized (via PCA)&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A scatter plot showing four distinct groups of colored dots (clusters). The clusters are well</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">separated, indicating that the K</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Means algorithm successfully found different patterns </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the data. The axes are labeled </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">PCA1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PCA2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span></span></code></pre></div><p>A scatter plot showing four distinct groups of colored dots (clusters). The clusters are well-separated, indicating that the K-Means algorithm successfully found different patterns in the data. The axes are labeled PCA1 and PCA2. Step 4: The &quot;So What?&quot; – Interpreting the Segments This is the most critical step for a Data Scientist. The algorithm outputs numbers (Cluster 0, 1, 2, 3). It is your job to translate those numbers into business logic. We do this by grouping our original data by the cluster ID and looking at the average values for Recency, Frequency, and Monetary.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Group by cluster and calculate the mean for each feature</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cluster_summary </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df.groupby(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).agg({</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Recency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Frequency&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Monetary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;count&#39;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # To see how many customers are in each group</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}).rename(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Cluster&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Count&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">})</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(cluster_summary.round(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p>Note: The cluster numbers (0, 1, 2, 3) are assigned randomly, so your specific numbers might vary, but the patterns will remain. Let’s assume the output looks like the table below: | Cluster | Recency (Days) | Frequency (Count) | Monetary ($) | Count | | :--- | :--- | :--- | :--- | :--- | | 0 | 245.50 | 2.10 | 150.00 | 120 | | 1 | 15.20 | 18.50 | 2800.00 | 105 | | 2 | 45.30 | 6.20 | 650.00 | 150 | | 3 | 180.10 | 15.00 | 2100.00 | 125 | Step 5: From Data to Strategy Now we wear our marketing hats. Let’s profile these customers and define a strategy for each. Cluster 1: The &quot;Champions&quot; (Low Recency, High Frequency, High Monetary) Profile: These customers shopped 15 days ago, buy often, and spend the most. Strategy: Retention. Do not send them discount coupons; they are already willing to pay full price. Instead, offer them exclusive access to new products, loyalty rewards, or a &quot;VIP&quot; status. Make them feel special. Cluster 0: The &quot;Lost Causes&quot; (High Recency, Low Frequency, Low Monetary) Profile: They haven&#39;t shopped in nearly a year (245 days), rarely bought when they did, and spent very little. Strategy: Deprioritize. Don&#39;t waste marketing budget here. Perhaps send a generic automated &quot;We miss you&quot; email, but focus your efforts elsewhere. Cluster 3: The &quot;At-Risk&quot; Whales (High Recency, High Frequency, High Monetary) Profile: This is a critical group! They used to buy frequently and spend a lot ($2100), but they haven&#39;t visited in 6 months (180 days). Something happened—they churned or went to a competitor. Strategy: Win-Back Campaign. This is where you spend your budget. Send aggressive discounts, &quot;Come back&quot; offers, or surveys to find out what went wrong. Winning them back is high-value. Cluster 2: The &quot;Promising&quot; Newbies (Medium Recency, Medium Frequency) Profile: They shop reasonably often and spend a decent amount. Strategy: Upsell/Cross-sell. Recommend related products to increase their average basket size. Try to nudge them into the &quot;Champion&quot; category. Summary In this case study, we moved beyond simple prediction. We didn&#39;t ask the computer &quot;Will this customer buy?&quot; (Supervised). Instead, we asked &quot;What kinds of customers do I have?&quot; (Unsupervised). By using K-Means Clustering, we transformed a wall of transaction numbers into four distinct human narratives. This allows the business to move from generic marketing to targeted, high-ROI strategies. This concludes our exploration of Unsupervised Learning. In the next chapter, we will tackle a completely different beast: dealing with text data and Natural Language Processing (NLP).</p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/chapters/chapter-8.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>8. Classification Algorithms</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/chapters/chapter-10.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>10. The Capstone Project</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Abish Pius</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"chapters_chapter-1.md\":\"DlMcsrFZ\",\"chapters_chapter-10.md\":\"BgAyvOIn\",\"chapters_chapter-11.md\":\"GIEFF29a\",\"chapters_chapter-12.md\":\"CY8_2Zte\",\"chapters_chapter-13.md\":\"C8sEq4-A\",\"chapters_chapter-14.md\":\"DxNMOqcT\",\"chapters_chapter-15.md\":\"ZQJY85LH\",\"chapters_chapter-16.md\":\"C2jiR1eu\",\"chapters_chapter-17.md\":\"DVcfmDVX\",\"chapters_chapter-18.md\":\"B7rTjQEU\",\"chapters_chapter-19.md\":\"DPXoCPhP\",\"chapters_chapter-2.md\":\"YUrB8e9Q\",\"chapters_chapter-20.md\":\"CcHxEGcG\",\"chapters_chapter-3.md\":\"B00f7XRF\",\"chapters_chapter-4.md\":\"cJlcpmpP\",\"chapters_chapter-5.md\":\"B5e_AoB8\",\"chapters_chapter-6.md\":\"d8_rf1rA\",\"chapters_chapter-7.md\":\"CS0POPcD\",\"chapters_chapter-8.md\":\"BxpaROns\",\"chapters_chapter-9.md\":\"BV5SxM8D\",\"chapters_intro.md\":\"D796NIDD\",\"index.md\":\"BkJeya3j\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"The Professional's Introduction to Data Science with Python\",\"description\":\"A comprehensive guide for career transitioners into Data Science with Python.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/logo.png\",\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Tutorial\",\"link\":\"/chapters/intro\"},{\"text\":\"About Author\",\"link\":\"https://abishpius.com\"}],\"sidebar\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"Welcome\",\"link\":\"/chapters/intro\"}]},{\"text\":\"The Book\",\"items\":[{\"text\":\"1. The Data Science Landscape\",\"link\":\"/chapters/chapter-1\"},{\"text\":\"2. Python Essentials\",\"link\":\"/chapters/chapter-2\"},{\"text\":\"3. Mastering Pandas\",\"link\":\"/chapters/chapter-3\"},{\"text\":\"4. Data Cleaning\",\"link\":\"/chapters/chapter-4\"},{\"text\":\"5. EDA & Visualization\",\"link\":\"/chapters/chapter-5\"},{\"text\":\"6. Statistical Foundations\",\"link\":\"/chapters/chapter-6\"},{\"text\":\"7. Predictive Modeling\",\"link\":\"/chapters/chapter-7\"},{\"text\":\"8. Classification Algorithms\",\"link\":\"/chapters/chapter-8\"},{\"text\":\"9. Pattern Discovery\",\"link\":\"/chapters/chapter-9\"},{\"text\":\"10. The Capstone Project\",\"link\":\"/chapters/chapter-10\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/abishpius\"}],\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Abish Pius\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>