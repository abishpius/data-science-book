<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Chapter 7: Predictive Modeling with Linear Regression | The Professional's Introduction to Data Science with Python</title>
    <meta name="description" content="A comprehensive guide for career transitioners into Data Science with Python.">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.C-O_2SOJ.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.BxNbBqlB.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.C7nZcP7s.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CDjunVez.js">
    <link rel="modulepreload" href="/assets/chapters_chapter-7.md.CS0POPcD.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>The Professional&#39;s Introduction to Data Science with Python</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/chapters/intro.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Tutorial</span><!--]--></a><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://abishpius.com" target="_blank" rel="noreferrer" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>About Author</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/abishpius" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/abishpius" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Introduction</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/intro.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Welcome</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>The Book</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-1.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>1. The Data Science Landscape</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-2.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>2. Python Essentials</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-3.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>3. Mastering Pandas</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-4.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>4. Data Cleaning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-5.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>5. EDA & Visualization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-6.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>6. Statistical Foundations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-7.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>7. Predictive Modeling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-8.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>8. Classification Algorithms</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-9.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>9. Pattern Discovery</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/chapters/chapter-10.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>10. The Capstone Project</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _chapters_chapter-7" data-v-39a288b8><div><h1 id="chapter-7-predictive-modeling-with-linear-regression" tabindex="-1">Chapter 7: Predictive Modeling with Linear Regression <a class="header-anchor" href="#chapter-7-predictive-modeling-with-linear-regression" aria-label="Permalink to &quot;Chapter 7: Predictive Modeling with Linear Regression&quot;">​</a></h1><p>Introduction to Scikit-Learn and the Modeling Workflow Up until this point in the book, we have functioned primarily as historians. We have cleaned historical records, visualized past trends, and used statistical tests to determine if past events were significant. We have been answering the question: What happened? Now, we cross the threshold into the most exciting part of Data Science: Predictive Modeling. We are shifting our focus from explaining the past to predicting the future. To do this, we will move beyond standard Python arithmetic and Pandas manipulation. We will introduce the industry-standard library for machine learning in Python: Scikit-Learn (often shortened to sklearn). The Machine Learning Paradigm Shift Before we write code, we must adjust our mental model. In traditional programming (like the cleaning scripts we wrote in Chapter 3), you provide the computer with Rules and Data, and it gives you Answers.</p><ul><li>Traditional Programming: If Marketing_Spend &gt; 1000, then Label = &quot;High Priority&quot;. In Machine Learning, we flip this. We provide the computer with Data and the Answers (historical results), and we ask the computer to learn the Rules.</li><li>Machine Learning: Here is how much we spent on marketing last year, and here is how much revenue we made. You tell me the mathematical relationship between them. The Vocabulary of Prediction To use Scikit-Learn effectively, you must become comfortable with its specific terminology. You will see these terms used in documentation, StackOverflow answers, and job interviews.</li></ul><ol><li>Target ($y$): This is what you are trying to predict. It is the &quot;Answer.&quot; In a spreadsheet, this is usually a single column. (e.g., Revenue, House Price, Customer Churn). 2. Features ($X$): These are the variables you use to make the prediction. These are the &quot;Inputs.&quot; (e.g., Marketing Spend, Square Footage, Number of Customer Support Calls). 3. Model: The mathematical engine that learns the relationship between $X$ and $y$. 4. Training: The process of letting the model look at your data to learn the rules. A diagram showing a standard Excel-style dataset. The last column is highlighted in Red and labeled &quot;Target (y) - What we want to predict&quot;. The first three columns are highlighted in Blue and labeled &quot;Features (X) - The data we use to predict&quot;. An arrow points from X to y labeled &quot;The Model learns this relationship&quot;.</li></ol><p>A diagram showing a standard Excel-style dataset. The last column is highlighted in Red and labeled &quot;Target (y) - What we want to predict&quot;. The first three columns are highlighted in Blue and labeled &quot;Features (X) - The data we use to predict&quot;. An arrow points from X to y labeled &quot;The Model learns this relationship&quot;. Note on Notation: In Python and Data Science conventions, we use a capital $X$ for features because it represents a matrix (multiple columns/dimensions), and a lowercase $y$ for the target because it represents a vector (a single column/dimension). Introducing Scikit-Learn Scikit-Learn is the most popular machine learning library for Python. It is open-source, robust, and incredibly consistent. The beauty of Scikit-Learn is its API consistency. Whether you are performing a simple Linear Regression (fitting a straight line) or a complex Random Forest (an ensemble of decision trees), the Python syntax remains nearly identical. Once you learn the &quot;Scikit-Learn Workflow,&quot; you can apply it to almost any algorithm. The 5-Step Modeling Workflow Every supervised machine learning project in Scikit-Learn follows this specific recipe. We will walk through the concepts first, and then apply them to code. Step 1: Arrange Data into Features ($X$) and Target ($y$) We must separate our DataFrame. We slice out the column we want to predict and save it as y. We select the columns we want to use for prediction and save them as X. Step 2: Train/Test Split This is the most critical concept for avoiding &quot;cheating.&quot; Imagine you are teaching a student (the Model) for a math exam. You give them a textbook containing 100 practice questions and the answers at the back. If the student memorizes the answers to all 100 questions, they will score 100% on a test if the test uses those exact same questions. However, if you give them a new question, they will fail. They didn&#39;t learn the math; they memorized the data. This is called Overfitting. To prevent this, we hide a portion of the data. 1. Training Set (e.g., 80% of data): The model is allowed to see this. It uses this to learn. 2. Testing Set (e.g., 20% of data): The model never sees this during training. We hold it back to evaluate how well the model performs on &quot;unseen&quot; data. A visual representation of the Train/Test Split. A horizontal bar representing a dataset is cut into two pieces. The larger piece (80%) is colored Green and labeled &quot;Training Set (Model learns from this)&quot;. The smaller piece (20%) is colored Orange and labeled &quot;Testing Set (Used to evaluate performance)&quot;. A &quot;No Peeking!&quot; icon separates the two.</p><p>A visual representation of the Train/Test Split. A horizontal bar representing a dataset is cut into two pieces. The larger piece (80%) is colored Green and labeled &quot;Training Set (Model learns from this)&quot;. The smaller piece (20%) is colored Orange and labeled &quot;Testing Set (Used to evaluate performance)&quot;. A &quot;No Peeking!&quot; icon separates the two. Step 3: Instantiate the Model We create an instance of the algorithm we want to use. In this chapter, we are using LinearRegression. Think of this as opening an empty box that has the capacity to learn, but hasn&#39;t learned anything yet. Step 4: Fit the Model This is the magic step. We command the model to &quot;Fit&quot; or &quot;Train&quot; on the Training Data. The model looks at the $X_{train}$ and compares it to the $y_{train}$ to calculate the best mathematical formula. Step 5: Predict and Evaluate Once the model is trained, we give it the $X_{test}$ (the exam questions without answers) and ask it to predict the $y$. We then compare its predictions against the actual $y_{test}$ values to grade its performance. Implementation in Python Let&#39;s apply this workflow to a simulated dataset. Imagine we are analyzing a retail business and want to predict Sales based on Marketing Budget. First, let&#39;s set up our data.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Creating a sample dataset</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Marketing_Budget&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">7000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">9000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Sales&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">22000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">25000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">29000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">35000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">46000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">56000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">61000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">65000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(data)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Display the first few rows</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(df.head(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p>Now, we follow the 5-step workflow using Scikit-Learn. Step 1: Separate X and y Note the double brackets [[&#39;Marketing_Budget&#39;]] for X. Scikit-Learn expects X to be a DataFrame (2D), even if it only has one column.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Step 1: Define Features (X) and Target (y)</span></span></code></pre></div><p>X = df[[&#39;Marketing_Budget&#39;]] # Features (Capital X, 2D array) y = df[&#39;Sales&#39;] # Target (Lowercase y, 1D array) Step 2: The Train/Test Split We use the train_test_split utility from Scikit-Learn. We will specify test_size=0.2 (holding back 20% of the data) and a random_state (to ensure your split looks the same as mine every time you run the code).</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Step 2: Split the data</span></span></code></pre></div><p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p><p>print(f&quot;Training records: {len(X_train)}&quot;) print(f&quot;Testing records: {len(X_test)}&quot;) Step 3 &amp; 4: Instantiate and Fit We import the LinearRegression class. This algorithm attempts to draw the &quot;Line of Best Fit&quot; through our data (we will explore the math of this line in the next section).</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.linear_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Step 3: Instantiate the model</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">lr_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Step 4: Fit the model (The Learning Phase)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">NOTE</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">: We only fit on the TRAINING data!</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">lr_model.fit(X_train, y_train)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Model has been trained.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Step 5: Predict Now that lr_model has learned the relationship between budget and sales, we can ask it to predict the sales for our test set, or even for a completely new budget number.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Step 5: Make predictions</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s predict the sales for the Test set (which the model hasn&#39;t seen)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">predictions </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> lr_model.predict(X_test)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s look at the results side-by-side</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">results </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: y_test, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Predicted&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: predictions})</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(results)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We can also predict for a hypothetical budget of $12,000</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">new_budget </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Double brackets for 2D shape</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">future_prediction </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> lr_model.predict(new_budget)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Predicted sales for $12k budget: $</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">future_prediction[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:,.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">IMAGE</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: A flowchart summarizing the code block above. 1. Box </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Raw Data&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> splits into </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;X (Features)&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;y (Target)&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. 2. Arrows lead to </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Train/Test Split&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. 3. </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Train Set&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> goes into </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Model.fit()&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. 4. </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Test Set&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> goes into </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Model.predict()&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. 5. The output of predict </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the actual Test labels meet at </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Evaluation/Comparison&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]]</span></span></code></pre></div><p>Summary You have just built your first Machine Learning pipeline. While the dataset was simple, the process is identical to what Data Scientists use at Google, Netflix, or Amazon:</p><ol><li>Isolate what you want to predict ($y$). 2. Split your data to prevent overfitting. 3. Initialize a model. 4. Train the model on the training set. 5. Use the model to make predictions. In the next section, we will lift the hood of the LinearRegression model to understand exactly how it calculated those predictions and how to interpret the &quot;Line of Best Fit&quot; for business stakeholders. Simple Linear Regression: Forecasting Continuous Variables In the previous section, we introduced Scikit-Learn and established the predictive modeling workflow: Instantiate, Fit, Predict. We are now ready to apply this workflow to the most fundamental algorithm in Data Science: Simple Linear Regression. While &quot;Linear Regression&quot; sounds like a dry statistical term, in a business context, it is a &quot;Crystal Ball generator.&quot; It allows us to move from saying &quot;Marketing and Sales are correlated&quot; (Descriptive) to saying &quot;If we increase the marketing budget by $1,000, Sales will increase by exactly $4,200&quot; (Predictive). The Geometry of Prediction At its core, Simple Linear Regression attempts to fit a straight line through your data points that best represents the relationship between two variables: 1. The Independent Variable ($X$): The input or driver (e.g., Marketing Spend). 2. The Dependent Variable ($y$): The output or target (e.g., Revenue). Imagine we have a scatter plot of last year&#39;s marketing campaigns. A scatter plot with &#39;Marketing Spend ($)&#39; on the x-axis and &#39;Revenue ($)&#39; on the y-axis. The data points show a positive trend, moving upward from left to right, indicating that as spend increases, revenue increases.</li></ol><p>A scatter plot with &#39;Marketing Spend ($)&#39; on the x-axis and &#39;Revenue ($)&#39; on the y-axis. The data points show a positive trend, moving upward from left to right, indicating that as spend increases, revenue increases. Our goal is to draw a line through these dots. However, we cannot just draw any line; we need the &quot;best&quot; line. But what defines &quot;best&quot;? The Mathematical Translation: $y = mx + b$ You likely remember the equation for a line from high school algebra: $y = mx + b$. In Data Science, we use slightly different notation, but the concept is identical: $$y = \beta_0 + \beta_1x$$ This equation is not just math; it is a business narrative.</p><ol><li>$y$ (Target): What we want to predict (Revenue). 2. $x$ (Feature): The lever we can pull (Marketing Spend). 3. $\beta_1$ (Coefficient/Slope): This is the most important number. It represents the impact. It tells us how much $y$ changes for every 1 unit increase in $x$. 4. $\beta_0$ (Intercept): This is the baseline. It represents the value of $y$ when $x$ is 0. (e.g., How much revenue would we make if we spent $0 on marketing? likely from word-of-mouth or existing contracts). A diagram illustrating the Linear Regression equation components on a chart. The &#39;Intercept&#39; is highlighted where the line crosses the Y-axis. The &#39;Slope&#39; is illustrated as a triangle stepping up along the line, labeled &#39;Rise over Run&#39; or &#39;Change in Y divided by Change in X&#39;.</li></ol><p>A diagram illustrating the Linear Regression equation components on a chart. The &#39;Intercept&#39; is highlighted where the line crosses the Y-axis. The &#39;Slope&#39; is illustrated as a triangle stepping up along the line, labeled &#39;Rise over Run&#39; or &#39;Change in Y divided by Change in X&#39;. How the Machine &quot;Learns&quot;: Minimizing Error When we ask Scikit-Learn to &quot;fit&quot; a model, it uses an algorithm called Ordinary Least Squares (OLS). Since real-world data is messy, no straight line will pass through every single data point perfectly. There will always be a gap between the actual data point and the predicted point on the line. This gap is called the Residual (or Error).</p><ul><li>The Goal: Find the specific line (slope and intercept) that makes the total sum of these squared errors as small as possible. A regression line cutting through scattered data points. Vertical lines are drawn connecting each data point to the regression line. These vertical lines are labeled &#39;Residuals&#39; or &#39;Errors&#39;.</li></ul><p>A regression line cutting through scattered data points. Vertical lines are drawn connecting each data point to the regression line. These vertical lines are labeled &#39;Residuals&#39; or &#39;Errors&#39;. If the line is too steep, the errors get large. If the line is too flat, the errors get large. The &quot;Best Fit Line&quot; sits right in the &quot;Goldilocks zone&quot; where error is minimized. Implementation in Python Let’s simulate a scenario. You are the Data Scientist for an e-commerce company. Your CMO (Chief Marketing Officer) gives you data on ad spend and revenue for the last 10 months and asks: &quot;If I spend $4,000 next month, exactly how much revenue should I expect?&quot; Here is how we solve this using Scikit-Learn.</p><ol><li>Prepare the Data First, we import our libraries and create the dataset.</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.linear_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Sample Data: Marketing Spend (X) and Revenue (y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;marketing_spend&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;revenue&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">18000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">23000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">29000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">34000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">40000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">47000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">53000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">58000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.read_csv(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;marketing_data.csv&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Assuming we loaded this from a file</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># SCALABILITY TIP: Scikit-Learn expects &#39;X&#39; (features) to be a 2D array (a table), </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># and &#39;y&#39; (target) to be a 1D array (a column).</span></span></code></pre></div><p>X = df[[&#39;marketing_spend&#39;]] # Double brackets make it a DataFrame (2D) y = df[&#39;revenue&#39;] # Single bracket makes it a Series (1D) 2. Instantiate and Fit We now initialize the algorithm and train it on our data.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Instantiate the model</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Fit the model (This is where OLS calculates the best line)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.fit(X, y)</span></span></code></pre></div><p>At this exact moment, Python has calculated the optimal $\beta_0$ and $\beta_1$. The &quot;learning&quot; is complete. 3. Extracting Insights Before we predict, we must interpret what the model learned. This is crucial for explaining the results to stakeholders.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">intercept </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.intercept_</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">coefficient </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.coef_[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Intercept (Baseline): $</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">intercept</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Coefficient (Marketing Impact): </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">coefficient</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><strong>Output:</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span>```text</span></span>
<span class="line"><span>Intercept (Baseline): $444.44</span></span></code></pre></div><p>Coefficient (Marketing Impact): 11.56 The Business Interpretation: The Baseline: Even if we turn off all marketing ads ($0 spend), our model estimates we would still make roughly $444 in revenue. The Impact: For every $1 dollar we add to the marketing budget, Revenue increases by $11.56. This is a powerful ROI metric to hand to your boss. 4. Making Predictions Finally, we answer the CMO&#39;s question: &quot;What happens if we spend $4,000?&quot;</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Predict revenue for a spend of $4000</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Note: We must pass the input as a 2D array, hence the double brackets [[ ]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">new_spend </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">predicted_revenue </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.predict(new_spend)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Projected Revenue for $4,000 spend: $</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">predicted_revenue[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:,.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><strong>Output:</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span>```text</span></span></code></pre></div><p>Projected Revenue for $4,000 spend: $46,666.67 Visualizing the Model To verify our work, we should visualize the original data against our new regression line.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.scatter(X, y, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">color</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;blue&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">label</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual Data&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.plot(X, model.predict(X), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">color</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;red&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">linewidth</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">label</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Regression Line&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Marketing Spend&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Revenue&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Simple Linear Regression: Spend vs Revenue&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.legend()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> The resulting plot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the code above. Blue dots represent the original data points. A bold red line cuts diagonally through the points, demonstrating a very close fit to the data.</span></span></code></pre></div><p>The resulting plot from the code above. Blue dots represent the original data points. A bold red line cuts diagonally through the points, demonstrating a very close fit to the data. Summary We have successfully built a Simple Linear Regression model. We moved from historical data to a mathematical equation. We used OLS to minimize the error of that equation. We interpreted the coefficient* to understand the &quot;exchange rate&quot; between marketing dollars and revenue. However, the real world is rarely driven by just one variable. Revenue isn&#39;t just driven by marketing; it&#39;s also driven by seasonality, competitor prices, and economic conditions. To handle that, we need to expand our toolkit to Multiple Linear Regression, which we will cover in the next section. Feature Engineering: Selecting the Right Predictors In the previous section, we built a &quot;Crystal Ball&quot; using Simple Linear Regression. We took a single input (Marketing Budget) and used it to forecast a single output (Sales). While this was a fantastic first step, your intuition as a business professional probably signaled a limitation. In the real world, outcomes are rarely driven by a single factor. If you are trying to predict the price of a house, you don&#39;t just look at the square footage. You also look at the number of bedrooms, the quality of the school district, the age of the roof, and the distance to the nearest highway. To build models that reflect the complexity of reality, we must graduate from Simple Linear Regression to Multiple Linear Regression. Instead of the formula looking like this: $$y = mx + b$$ It now looks like this: $$y = m_1x_1 + m_2x_2 + m_3x_3 + ... + b$$ Where $x_1$, $x_2$, and $x_3$ are different features (predictors) in your dataset. However, adding more data brings a new challenge. Just because you have the data doesn&#39;t mean you should use it. This section focuses on Feature Selection: the art and science of choosing the right inputs to prevent your model from becoming confused, slow, or inaccurate. The &quot;Kitchen Sink&quot; Trap A common mistake for those transitioning into Data Science is the &quot;Kitchen Sink&quot; approach: throwing every available column of data into the model hoping it finds a pattern. Imagine you are hiring a Sales Manager. You have a stack of resumes. To predict who will be the best hire, you look at: 1. Years of experience. 2. Past sales figures. 3. Industry contacts. But would you also look at their shoe size? Or the day of the week they were born? Or their favorite ice cream flavor? Obviously not. Those variables are noise. If you force a mathematical model to find a relationship between &quot;Shoe Size&quot; and &quot;Sales Performance,&quot; it might accidentally find a coincidental pattern in your historical data. When you try to use that model on a new candidate, the prediction will fail because the relationship wasn&#39;t real. A split illustration. On the left, a funnel labeled &quot;All Data&quot; pouring into a machine, resulting in a graph with messy, erratic lines labeled &quot;Overfitting/Noise&quot;. On the right, a filter labeled &quot;Feature Selection&quot; blocking irrelevant data (like &quot;Shoe Size&quot;) while letting relevant data (like &quot;Ad Spend&quot;) through to the machine, resulting in a clean, straight trend line.</p><p>A split illustration. On the left, a funnel labeled &quot;All Data&quot; pouring into a machine, resulting in a graph with messy, erratic lines labeled &quot;Overfitting/Noise&quot;. On the right, a filter labeled &quot;Feature Selection&quot; blocking irrelevant data (like &quot;Shoe Size&quot;) while letting relevant data (like &quot;Ad Spend&quot;) through to the machine, resulting in a clean, straight trend line. The Two Golden Rules of Feature Selection When selecting features for Linear Regression, we are generally looking for two things:</p><ol><li>High Correlation with the Target: The feature should move in sync with what we are trying to predict. (e.g., As House Size goes up, Price goes up). 2. Low Correlation with Other Features: The features should be independent of one another. We already understand Rule #1. Rule #2, however, is where many data scientists stumble. It introduces a concept called Multicollinearity. Understanding Multicollinearity Multicollinearity occurs when two or more predictors in your model are highly correlated with each other. They are essentially providing the model with the same information. Imagine we are trying to predict the total revenue of a lemonade stand. Feature A: Number of cups sold. Feature B: Amount of revenue from cups sold. If we include both A and B in the model to predict Total Revenue, the model gets confused. It doesn&#39;t know which variable to assign the &quot;credit&quot; to. Mathematically, this makes the model unstable. The coefficients (the $m$ in our equation) can swing wildly, making the model difficult to interpret. Business Analogy: It is like having two employees work on the exact same task, but not telling them about each other. They might both do the work, or they might get in each other&#39;s way, and you end up paying double the salary for the same output. Practical Workflow: The Correlation Matrix How do we find the right features and avoid multicollinearity? We use a Correlation Matrix. Let&#39;s look at a dataset for a hypothetical E-commerce company. We want to predict Yearly Amount Spent (Target). Our available features are: <code>Avg. Session Length</code>: Average time a user stays on the site. Time on App: Average time spent on the mobile app. <code>Time on Website</code>: Average time spent on the desktop site. Length of Membership: How many years they have been a customer. Here is how we visualize these relationships using Python and the library seaborn.</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seaborn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sns</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Load the dataset</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">customers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.read_csv(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Ecommerce_Customers.csv&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Calculate the correlation matrix</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># .corr() computes the pairwise correlation of columns</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">correlation_matrix </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> customers.corr()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Visualize with a Heatmap</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sns.heatmap(correlation_matrix, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">annot</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cmap</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;coolwarm&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Correlation Matrix of E-Commerce Features&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A heatmap generated by Python. The diagonal shows dark red squares (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> correlation). The </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Yearly Amount Spent&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> row shows varying shades. </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Length of Membership&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> has a dark red square (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) indicating high correlation. </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Time on Website&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> is</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a light blue square (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.02</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) indicating almost no correlation. </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Time on App&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> is</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a medium red (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).</span></span></code></pre></div><p>A heatmap generated by Python. The diagonal shows dark red squares (1.0 correlation). The &quot;Yearly Amount Spent&quot; row shows varying shades. &quot;Length of Membership&quot; has a dark red square (0.8) indicating high correlation. &quot;Time on Website&quot; is a light blue square (0.02) indicating almost no correlation. &quot;Time on App&quot; is a medium red (0.5). Interpreting the Heatmap When you run this code, you will generate a grid of colored squares. Here is how to read it to select your features:</p><ol><li>Look at the Target Row/Column: Find the row labeled Yearly Amount Spent. Look for colors indicating strong correlation (values close to 1.0 or -1.0). Observation: In our hypothetical plot, <code>Length of Membership</code> has a correlation of 0.8. This is a fantastic predictor. <code>Time on App</code> is 0.5. Good. <code>Time on Website</code> is -0.02*. This is noise; it has no relationship with spending. We should likely drop Time on Website.</li><li>Check for Multicollinearity: Look at the intersection of your features. Observation:* Check the intersection of Time on App and Length of Membership. If this value was very high (e.g., 0.9), we would have a problem (Multicollinearity). We would need to delete one of them. In this case, let&#39;s assume they are not correlated. Implementing Multiple Linear Regression Once we have selected our features (let&#39;s say we chose Length of Membership and Time on App), we simply update our X variable. In Simple Linear Regression, X was a single column (a Pandas Series or 1D array). In Multiple Linear Regression, X is a DataFrame (a matrix).</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.linear_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Select Features (Predictors) and Target</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We drop &#39;Time on Website&#39; because of low correlation seen in the heatmap</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Length of Membership&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Time on App&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;Yearly Amount Spent&#39;</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> customers[features]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> customers[target]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Split the data (Standard Practice)</span></span></code></pre></div><p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</p><h1 id="_3-instantiate-the-model" tabindex="-1">3. Instantiate the model <a class="header-anchor" href="#_3-instantiate-the-model" aria-label="Permalink to &quot;3. Instantiate the model&quot;">​</a></h1><p>lm = LinearRegression()</p><h1 id="_4-fit-the-model" tabindex="-1">4. Fit the model <a class="header-anchor" href="#_4-fit-the-model" aria-label="Permalink to &quot;4. Fit the model&quot;">​</a></h1><h1 id="scikit-learn-handles-multiple-features-automatically" tabindex="-1">Scikit-Learn handles multiple features automatically! <a class="header-anchor" href="#scikit-learn-handles-multiple-features-automatically" aria-label="Permalink to &quot;Scikit-Learn handles multiple features automatically!&quot;">​</a></h1><p>lm.fit(X_train, y_train)</p><h1 id="_5-inspect-the-coefficients" tabindex="-1">5. Inspect the Coefficients <a class="header-anchor" href="#_5-inspect-the-coefficients" aria-label="Permalink to &quot;5. Inspect the Coefficients&quot;">​</a></h1><p>print(f&quot;Intercept: {lm.intercept_}&quot;) print(f&quot;Coefficients: {lm.coef_}&quot;) Output Interpretation: The lm.coef_ will now print an array with two numbers, corresponding to the order of features you provided (Length of Membership, then Time on App). If the output is [63.5, 38.2], the equation is: $$Spending = 63.5 \times (\text{Membership Years}) + 38.2 \times (\text{App Time}) + \text{Intercept}$$ The Business Translation: &quot;For every one year increase in Membership, spending increases by $63.50, holding all other factors constant.&quot; &quot;For every one hour increase in App Time, spending increases by $38.20, holding all other factors constant.&quot; Note the phrase &quot;holding all other factors constant.&quot; This is the power of Multiple Linear Regression. It isolates the impact of one specific variable while accounting for the noise and influence of the others. Summary We have moved from the simple world of single-variable prediction to the complex reality of multi-variable environments. By using Feature Selection, we ensure our model focuses on the signal and ignores the noise. By checking for Multicollinearity, we ensure our predictors aren&#39;t fighting each other for credit. But we still have a lingering question. We built a model, and it produced an equation. But how do we know if the model is actually good? How accurate is it? In the next section, we will learn the critical metrics for Model Evaluation. Case Study: Predicting Real Estate Prices We have arrived at the convergence of theory and practice. In the previous sections, we learned the mechanics of Simple Linear Regression (one input, one output) and discussed Feature Engineering (the art of selecting meaningful inputs). Now, we will combine these concepts to solve a classic, real-world business problem: Automated Valuation Models (AVMs). If you have ever used Zillow or Redfin to check the estimated value of a home, you have interacted with the technology we are about to build. As a career-transitioning Data Scientist, you must move beyond just fitting a line to a scatter plot. You must now manage the entire modeling lifecycle: loading data, selecting multiple features, splitting data to prevent cheating (overfitting), training the model, and—crucially—explaining the results to a business stakeholder. The Business Problem Imagine you have been hired by a Real Estate Investment Trust (REIT). They want to automate the bidding process for houses. Their current manual process takes too long, causing them to lose deals. They have provided you with a historical dataset of 1,000 homes sold in the last year, including details like square footage, number of bedrooms, age of the home, and the final selling price. Your task: Build a model that predicts the SalePrice based on the property&#39;s characteristics. Step 1: Loading and Inspecting the Data First, we load our libraries. We will use pandas for data handling and scikit-learn for the heavy lifting.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seaborn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sns</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.linear_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.metrics </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mean_absolute_error, r2_score</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Load the dataset</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># For this case study, we assume a cleaned CSV file named &#39;housing_data.csv&#39;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.read_csv(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;housing_data.csv&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Inspect the first few rows to understand our ingredients</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(df.head())</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Output (Simulated):</span></span></code></pre></div><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span></code></pre></div><p>Square_Feet Bedrooms Bathrooms Year_Built Neighborhood SalePrice 0 2100 4 2.5 1998 Suburb_A 320000 1 1600 3 2.0 1985 Suburb_B 250000 2 2800 4 3.0 2010 Suburb_A 410000 3 1200 2 1.0 1960 Urban_C 180000 4 1900 3 2.5 2005 Suburb_B 295000 Step 2: Feature Selection and Correlation In the previous section on Feature Engineering, we discussed that not all data points are useful predictors. A &quot;Case Study ID&quot; or &quot;Homeowner Name&quot; has no bearing on the market value of a house. We need to select features that have a statistical relationship with SalePrice. A correlation matrix is our best tool here. A heatmap visualization generated by Seaborn. The X and Y axes list variables: Square_Feet, Bedrooms, Bathrooms, Year_Built, and SalePrice. The intersection of Square_Feet and SalePrice is colored dark red with a coefficient of 0.85, indicating strong positive correlation. The intersection of Bedrooms and SalePrice is moderately red (0.55).</p><p>A heatmap visualization generated by Seaborn. The X and Y axes list variables: Square_Feet, Bedrooms, Bathrooms, Year_Built, and SalePrice. The intersection of Square_Feet and SalePrice is colored dark red with a coefficient of 0.85, indicating strong positive correlation. The intersection of Bedrooms and SalePrice is moderately red (0.55). Based on our EDA (Exploratory Data Analysis), we observe that Square_Feet has the strongest relationship with price, but Year_Built and Bathrooms are also significant. We will define our Feature Matrix (X) and Target Vector (y). Note: In this specific case study, we will focus on numerical features for Multiple Linear Regression. Handling categorical text data (like &#39;Neighborhood&#39;) requires a technique called One-Hot Encoding, which we will cover in Chapter 8.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Define our features (Inputs)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We use double brackets [[]] to create a DataFrame, not a Series</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Square_Feet&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Bedrooms&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Bathrooms&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Year_Built&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[features]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Define our target (Output)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;SalePrice&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><p>Step 3: The Train-Test Split This is the most critical conceptual leap from &quot;Statistics&quot; to &quot;Machine Learning.&quot; If we show our model all the data during training, it will memorize the answers. It&#39;s like giving a student the answer key to the exam before they take it. They will get a 100% score, but they won&#39;t have learned how to solve the problems. To assess if our model works on new, unseen houses, we split our data into two sets: 1. Training Set (80%): Used to teach the model. 2. Testing Set (20%): Locked away in a &quot;vault.&quot; We only use this to grade the model at the very end. A diagram illustrating the Train-Test Split. A large rectangle represents the full dataset. It is sliced vertically. The left side (80%) is blue and labeled &quot;Training Data (Model learns from this)&quot;. The right side (20%) is orange and labeled &quot;Testing Data (Used for evaluation)&quot;. Arrows verify that the Model never sees the orange data until the prediction phase.</p><p>A diagram illustrating the Train-Test Split. A large rectangle represents the full dataset. It is sliced vertically. The left side (80%) is blue and labeled &quot;Training Data (Model learns from this)&quot;. The right side (20%) is orange and labeled &quot;Testing Data (Used for evaluation)&quot;. Arrows verify that the Model never sees the orange data until the prediction phase.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Split the data</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># random_state=42 ensures we get the same split every time we run the code (reproducibility)</span></span></code></pre></div><p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p><p>print(f&quot;Training samples: {len(X_train)}&quot;) print(f&quot;Testing samples: {len(X_test)}&quot;) Step 4: Instantiating and Fitting the Model Now we follow the Scikit-Learn workflow introduced earlier. Notice that the code for Multiple Linear Regression is identical to Simple Linear Regression. The algorithm handles the math of balancing multiple variables (coefficients) automatically.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Instantiate the model</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LinearRegression()</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Fit the model (Learn the patterns)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># IMPORTANT: We fit ONLY on the training data</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.fit(X_train, y_train)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Model training complete.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>At this exact moment, the model object has calculated the &quot;Line of Best Fit&quot;—or rather, the &quot;Hyperplane of Best Fit&quot; since we are working in multiple dimensions. It has learned how much weight to give Square_Feet versus Bedrooms. Step 5: Evaluating Performance How accurate is our crystal ball? To find out, we ask the model to predict the prices for the homes in our Testing Set (X_test). We then compare those predictions to the actual prices (y_test) which we hid from the model.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. Predict</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">predictions </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.predict(X_test)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 4. Evaluate</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mae </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mean_absolute_error(y_test, predictions)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">r2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> r2_score(y_test, predictions)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Mean Absolute Error (MAE): $</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mae</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:,.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;R-squared Score: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">r2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.2f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Output (Simulated):</span></span></code></pre></div><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span></code></pre></div><p>Mean Absolute Error (MAE): $18,450.00 R-squared Score: 0.82 Interpreting the Metrics for Business Leaders As a Data Scientist, you cannot simply email the CEO saying &quot;The R-squared is 0.82.&quot; You must translate this.</p><ul><li>The Translation: &quot;Our model explains 82% of the variation in housing prices. On average, our automated price estimates are within $18,450 of the actual selling price.&quot;</li><li>The Decision: The business must decide if an error margin of ~$18k is acceptable. If they are flipping high-end luxury homes, this is excellent accuracy. If they are buying $50k fixer-uppers, this margin of error might be too high. Step 6: Visualizing the Results Numbers are abstract; visuals are persuasive. A common way to diagnose a regression model is plotting Actual vs. Predicted values.</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sns.scatterplot(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_test, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">predictions)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Draw a red line representing perfect prediction</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">color</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;red&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">lw</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual Prices&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Predicted Prices&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual vs. Predicted Housing Prices&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A scatter plot showing </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual Prices&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;Predicted Prices&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> on the Y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">axis. The dots cluster tightly around a red diagonal line running </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> bottom</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">left to top</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">right. This indicates high accuracy. A few outlier dots are far </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> the line, representing homes where the model predicted poorly.</span></span></code></pre></div><p>A scatter plot showing &#39;Actual Prices&#39; on the X-axis and &#39;Predicted Prices&#39; on the Y-axis. The dots cluster tightly around a red diagonal line running from bottom-left to top-right. This indicates high accuracy. A few outlier dots are far from the line, representing homes where the model predicted poorly. If the dots fall exactly on the red line, the prediction is perfect. Dots significantly above or below the line represent errors. This visual helps you quickly identify if your model is failing on specific types of houses (e.g., maybe it consistently undervalues expensive mansions). Step 7: The &quot;Why&quot; – Interpreting Coefficients Finally, we look inside the &quot;Black Box.&quot; Because this is Linear Regression, we can see exactly how the model makes decisions by looking at the coefficients.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">coef_df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(model.coef_, features, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Coefficient&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(coef_df)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Output (Simulated):</span></span></code></pre></div><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Coefficient</span></span>
<span class="line"><span>Square_Feet       150.25</span></span>
<span class="line"><span>Bedrooms        -5000.00</span></span>
<span class="line"><span>Bathrooms       12000.00</span></span>
<span class="line"><span>Year_Built        800.00</span></span></code></pre></div><p>Wait, why is the coefficient for Bedrooms negative? This is a common analytical trap. It suggests that, holding all other variables constant, adding a bedroom reduces the price by $5,000. Mathematically, this happens because Square_Feet is already in the model. If you have two houses that are both 2,000 sq ft, but one has 3 bedrooms and the other has 5 bedrooms, the 5-bedroom house must have extremely tiny rooms (chopped up space), which might be less desirable. This reinforces the lesson from the Correlation vs. Causation section: model coefficients describe mathematical relationships, not necessarily physical laws. Summary In this case study, you successfully: 1. Loaded and inspected raw data. 2. Selected features based on correlation. 3. Split data to validate performance on unseen records. 4. Trained a Multiple Linear Regression model. 5. Translated technical metrics (MAE) into business risk (Dollars). You have moved from describing the past to predicting the future. However, you may have noticed that our model assumed the relationship between size and price is a straight line. What if the relationship is curved? What if location matters more than size? In the next chapter, we will explore how to handle these complexities using non-linear models.</p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/chapters/chapter-6.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>6. Statistical Foundations</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/chapters/chapter-8.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>8. Classification Algorithms</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Abish Pius</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"chapters_chapter-1.md\":\"DlMcsrFZ\",\"chapters_chapter-10.md\":\"BgAyvOIn\",\"chapters_chapter-11.md\":\"GIEFF29a\",\"chapters_chapter-12.md\":\"CY8_2Zte\",\"chapters_chapter-13.md\":\"C8sEq4-A\",\"chapters_chapter-14.md\":\"DxNMOqcT\",\"chapters_chapter-15.md\":\"ZQJY85LH\",\"chapters_chapter-16.md\":\"C2jiR1eu\",\"chapters_chapter-17.md\":\"DVcfmDVX\",\"chapters_chapter-18.md\":\"B7rTjQEU\",\"chapters_chapter-19.md\":\"DPXoCPhP\",\"chapters_chapter-2.md\":\"YUrB8e9Q\",\"chapters_chapter-20.md\":\"CcHxEGcG\",\"chapters_chapter-3.md\":\"B00f7XRF\",\"chapters_chapter-4.md\":\"cJlcpmpP\",\"chapters_chapter-5.md\":\"B5e_AoB8\",\"chapters_chapter-6.md\":\"d8_rf1rA\",\"chapters_chapter-7.md\":\"CS0POPcD\",\"chapters_chapter-8.md\":\"BxpaROns\",\"chapters_chapter-9.md\":\"BV5SxM8D\",\"chapters_intro.md\":\"D796NIDD\",\"index.md\":\"BkJeya3j\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"The Professional's Introduction to Data Science with Python\",\"description\":\"A comprehensive guide for career transitioners into Data Science with Python.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/logo.png\",\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Tutorial\",\"link\":\"/chapters/intro\"},{\"text\":\"About Author\",\"link\":\"https://abishpius.com\"}],\"sidebar\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"Welcome\",\"link\":\"/chapters/intro\"}]},{\"text\":\"The Book\",\"items\":[{\"text\":\"1. The Data Science Landscape\",\"link\":\"/chapters/chapter-1\"},{\"text\":\"2. Python Essentials\",\"link\":\"/chapters/chapter-2\"},{\"text\":\"3. Mastering Pandas\",\"link\":\"/chapters/chapter-3\"},{\"text\":\"4. Data Cleaning\",\"link\":\"/chapters/chapter-4\"},{\"text\":\"5. EDA & Visualization\",\"link\":\"/chapters/chapter-5\"},{\"text\":\"6. Statistical Foundations\",\"link\":\"/chapters/chapter-6\"},{\"text\":\"7. Predictive Modeling\",\"link\":\"/chapters/chapter-7\"},{\"text\":\"8. Classification Algorithms\",\"link\":\"/chapters/chapter-8\"},{\"text\":\"9. Pattern Discovery\",\"link\":\"/chapters/chapter-9\"},{\"text\":\"10. The Capstone Project\",\"link\":\"/chapters/chapter-10\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/abishpius\"}],\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Abish Pius\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>