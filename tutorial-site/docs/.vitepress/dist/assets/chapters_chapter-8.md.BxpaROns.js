import{_ as i,c as e,o as a,ae as t}from"./chunks/framework.CDjunVez.js";const c=JSON.parse('{"title":"Chapter 8: Classification Algorithms for Categorical Outcomes","description":"","frontmatter":{},"headers":[],"relativePath":"chapters/chapter-8.md","filePath":"chapters/chapter-8.md"}'),n={name:"chapters/chapter-8.md"};function l(h,s,o,r,p,k){return a(),e("div",null,[...s[0]||(s[0]=[t(`<h1 id="chapter-8-classification-algorithms-for-categorical-outcomes" tabindex="-1">Chapter 8: Classification Algorithms for Categorical Outcomes <a class="header-anchor" href="#chapter-8-classification-algorithms-for-categorical-outcomes" aria-label="Permalink to &quot;Chapter 8: Classification Algorithms for Categorical Outcomes&quot;">​</a></h1><p>Logistic Regression: Predicting Binary Outcomes In the previous chapters, we built a foundation for Regression: predicting a continuous number. We answered questions like &quot;What will the house price be?&quot; or &quot;How many units will we sell next quarter?&quot; But in the business world, not every question is about &quot;How much?&quot; Often, the most critical questions are about &quot;Which one?&quot; or &quot;Yes or No?&quot;</p><ul><li>Will this customer churn? (Yes/No)</li><li>Is this transaction fraudulent? (Yes/No)</li><li>Will this lead convert into a sale? (Yes/No) This brings us to the second pillar of Supervised Learning: Classification. Specifically, we will look at the algorithm used to predict binary outcomes: Logistic Regression. Despite its confusing name (it includes the word &quot;Regression&quot;), Logistic Regression is used strictly for Classification. It is the industry standard for calculating the probability of an event occurring. The Problem with Linear Regression for Classification To understand why we need a new algorithm, let’s imagine we are trying to predict if a customer will buy a new product based on their age. In our dataset, the outcome ($y$) is binary: $0$ = Did not buy $1$ = Bought If we attempted to use the Simple Linear Regression model we mastered in previous sections, we would try to draw a straight line through this data. A scatter plot showing &#39;Age&#39; on the X-axis and &#39;Purchased&#39; on the Y-axis. The data points are clustered strictly at Y=0 and Y=1. A straight blue regression line attempts to cut through the data diagonally, extending below 0 and above 1.</li></ul><p>A scatter plot showing &#39;Age&#39; on the X-axis and &#39;Purchased&#39; on the Y-axis. The data points are clustered strictly at Y=0 and Y=1. A straight blue regression line attempts to cut through the data diagonally, extending below 0 and above 1. There are two major problems with applying a straight line here: 1. The bounds are broken: A straight line extends to infinity in both directions. For a very old customer, the model might predict a value of $1.5$. For a very young customer, it might predict $-0.4$. In the context of probability, what does &quot;150% chance of buying&quot; or &quot;-40% chance of buying&quot; mean? It is mathematically impossible. 2. The relationship isn&#39;t linear: In binary decisions, the change often happens quickly around a threshold. A small increase in age might not matter much until a tipping point is reached, at which point the likelihood of purchase spikes. We need a model that bounds our output between 0 and 1 and handles that &quot;tipping point.&quot; The Solution: The Sigmoid Function To solve this, Logistic Regression takes the straight line equation ($y = mx + b$) and wraps it inside a transformation function called the Sigmoid Function (also known as the Logistic Function). Without getting bogged down in the calculus, the Sigmoid function acts like a squashing machine. It takes any number—no matter how large or small—and squashes it into a value between 0 and 1. Visually, this turns our straight line into an S-curve. A graph showing the Sigmoid S-curve. The X-axis represents the input (e.g., Age), and the Y-axis represents Probability ranging from 0.0 to 1.0. The curve starts flat near 0, rises steeply in the middle, and flattens out near 1.</p><p>A graph showing the Sigmoid S-curve. The X-axis represents the input (e.g., Age), and the Y-axis represents Probability ranging from 0.0 to 1.0. The curve starts flat near 0, rises steeply in the middle, and flattens out near 1. This S-curve is perfect for probabilities. If the input is very low, the curve flattens at 0 (0% probability). If the input is very high, the curve flattens at 1 (100% probability). * The steep slope in the middle represents the transition zone where the outcome is uncertain. From Probability to Prediction When you run a Logistic Regression model, the raw output is a Probability Score. Customer A: 0.92 (92% likely to buy) Customer B: 0.15 (15% likely to buy) * Customer C: 0.51 (51% likely to buy) However, a computer needs to make a binary decision. To convert this probability into a class label ($0$ or $1$), we apply a Threshold (also called a decision boundary). The default threshold in Scikit-Learn is 0.5. If Probability $&gt; 0.5$: Predict Class 1 (Yes) If Probability $\\leq 0.5$: Predict Class 0 (No) Note: In advanced business applications, you might tune this threshold. For example, in cancer detection, you might lower the threshold to 0.1 because you want to flag anything that is even remotely suspicious. But for now, we will stick to the default. Implementation in Scikit-Learn Let&#39;s apply this to a dataset. We will generate a synthetic dataset representing customers, their age, and whether they purchased a specific insurance policy. The workflow remains the same as it was for Linear Regression: Instantiate, Fit, Predict.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.linear_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LogisticRegression</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Create sample data (Age vs. Purchased)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Synthetic data: Older people are more likely to buy</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Age&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">22</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">25</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">47</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">52</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">46</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">56</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">55</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">60</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">62</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">61</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">18</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">28</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">27</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">29</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">49</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Purchased&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(data)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Split features (X) and target (y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Age&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Purchased&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. Train/Test Split</span></span></code></pre></div><p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p><h1 id="_4-instantiate-the-model" tabindex="-1">4. Instantiate the model <a class="header-anchor" href="#_4-instantiate-the-model" aria-label="Permalink to &quot;4. Instantiate the model&quot;">​</a></h1><h1 id="note-we-use-logisticregression-not-linearregression" tabindex="-1">Note: We use LogisticRegression, not LinearRegression <a class="header-anchor" href="#note-we-use-logisticregression-not-linearregression" aria-label="Permalink to &quot;Note: We use LogisticRegression, not LinearRegression&quot;">​</a></h1><p>log_reg = LogisticRegression()</p><h1 id="_5-fit-the-model" tabindex="-1">5. Fit the model <a class="header-anchor" href="#_5-fit-the-model" aria-label="Permalink to &quot;5. Fit the model&quot;">​</a></h1><p>log_reg.fit(X_train, y_train)</p><h1 id="_6-make-predictions" tabindex="-1">6. Make Predictions <a class="header-anchor" href="#_6-make-predictions" aria-label="Permalink to &quot;6. Make Predictions&quot;">​</a></h1><p>predictions = log_reg.predict(X_test)</p><p>print(f&quot;Test Set Ages: \\n{X_test.values.flatten()}&quot;) print(f&quot;Predictions (0=No, 1=Yes): {predictions}&quot;) <strong>Output:</strong></p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Test Set Ages: </span></span>
<span class="line"><span>[28 22 25]</span></span></code></pre></div><p>Predictions (0=No, 1=Yes): [0 0 0] In this small test set, the model predicted &#39;0&#39; (No purchase) for all three young customers. This aligns with the trend in our data. Peeking Under the Hood: predict_proba As a Data Scientist, the binary prediction is useful, but the probability is often more valuable for business strategy. Knowing a customer is 51% likely to leave is very different from knowing they are 99% likely to leave, even though both result in a prediction of &quot;Churn.&quot; Scikit-Learn allows us to see these probabilities using the method .predict_proba().</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Predict probabilities for the test set</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">probabilities </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> log_reg.predict_proba(X_test)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Display nicely</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">results </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(probabilities, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">columns</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Prob_No&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Prob_Yes&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">results[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Age&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X_test.values</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">results[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Predicted_Class&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> predictions</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(results)</span></span></code></pre></div><p><strong>Output:</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span>\`\`\`text</span></span></code></pre></div><p>Prob_No Prob_Yes Age Predicted_Class 0 0.8124 0.1876 28 0 1 0.9045 0.0955 22 0 2 0.8650 0.1350 25 0 Here, we can see the nuance. The 22-year-old had a 9.5% probability of buying (Prob_Yes), while the 28-year-old had an 18.7% probability. Neither crossed the 50% threshold, so both were classified as 0. A visualization of the output dataframe above. Arrows point from the &#39;Prob_Yes&#39; column to the &#39;Predicted_Class&#39; column, illustrating that since Prob_Yes &lt; 0.5, the Class is 0.</p><p>A visualization of the output dataframe above. Arrows point from the &#39;Prob_Yes&#39; column to the &#39;Predicted_Class&#39; column, illustrating that since Prob_Yes &lt; 0.5, the Class is 0. Interpreting Coefficients in Logistic Regression In Linear Regression, we learned that the coefficient told us exactly how much $y$ increased for every unit of $x$. In Logistic Regression, interpretation is slightly more complex because of the Sigmoid transformation. The coefficients represent the log-odds, which is not intuitive for most business stakeholders. However, the sign (positive or negative) of the coefficient is immediately useful:</p><ol><li>Positive Coefficient: As the feature increases, the probability of the event (1) increases. 2. Negative Coefficient: As the feature increases, the probability of the event (1) decreases.</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Coefficient for Age: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">log_reg.coef_[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>If this prints a positive number (e.g., 0.15), it confirms that as Age goes up, the likelihood of Purchasing goes up. Summary We have now moved from predicting values (Linear Regression) to predicting probabilities and classes (Logistic Regression).</p><ul><li>Linear Regression is for continuous outcomes (Price, Temperature, Sales).</li><li>Logistic Regression is for binary categories (Yes/No, True/False).</li><li>The Sigmoid function transforms the output into a probability between 0 and 1.</li><li>A Threshold (usually 0.5) turns that probability into a hard decision. However, making a prediction is only half the battle. How do we know if our classification model is actually good? In Regression, we looked at the error distance (RMSE). In Classification, &quot;distance&quot; doesn&#39;t make sense—you are either right or wrong. In the next section, we will explore the Confusion Matrix and Accuracy Scores to evaluate the performance of our classification models. Decision Trees: Mapping Logic to Predictions In the previous section, we explored Logistic Regression, a powerful method for predicting binary outcomes (Yes/No). We learned that despite its name, it is a classification algorithm that draws an &quot;S-curve&quot; (the Sigmoid function) to separate classes based on probability. While Logistic Regression is fantastic for understanding relationships (e.g., &quot;How does increasing price affect the probability of a sale?&quot;), it has a distinct limitation: it assumes a mathematical, linear relationship between the features and the log-odds of the outcome. But human decision-making rarely follows a smooth mathematical curve. When you decide whether to wear a coat, you don&#39;t calculate a probability coefficient. You follow a set of logic rules: 1. Is it raining? If Yes -&gt; Wear a coat. 2. If No, is it below 60 degrees? If Yes -&gt; Wear a coat. 3. If No -&gt; Don&#39;t wear a coat. This logic—a series of sequential questions leading to a conclusion—is the foundation of the Decision Tree. The Intuition: The &quot;Flowchart&quot; Model If you have ever followed a Standard Operating Procedure (SOP) or a troubleshooting guide at work, you have manually executed a Decision Tree. In Data Science, a Decision Tree is a supervised learning algorithm that splits your data into smaller and smaller subsets based on specific criteria. It &quot;grows&quot; an upside-down tree structure:</li></ul><ol><li>The Root Node: The starting point containing the entire dataset. 2. Decision Nodes: Points where the data is split based on a specific variable (e.g., &quot;Income &gt; $50k&quot;). 3. Leaf Nodes: The endpoints where a final prediction is made. A diagram of a decision tree structure. The top box is labeled &#39;Root Node (All Data)&#39;. Arrows branch out to &#39;Decision Nodes&#39; containing questions like &#39;Credit Score &gt; 700?&#39;. The bottom boxes are labeled &#39;Leaf Nodes&#39; containing the final classifications &#39;Approve Loan&#39; and &#39;Deny Loan&#39;.</li></ol><p>A diagram of a decision tree structure. The top box is labeled &#39;Root Node (All Data)&#39;. Arrows branch out to &#39;Decision Nodes&#39; containing questions like &#39;Credit Score &gt; 700?&#39;. The bottom boxes are labeled &#39;Leaf Nodes&#39; containing the final classifications &#39;Approve Loan&#39; and &#39;Deny Loan&#39;. Unlike the &quot;Black Box&quot; nature of some advanced algorithms (where the math is so complex it is hard to explain why a prediction was made), Decision Trees are White Box models. They are completely transparent. If your boss asks, &quot;Why did the model reject this loan application?&quot;, you can trace the exact path down the tree: &quot;Because the applicant&#39;s income was low AND their debt-to-income ratio was high.&quot; How the Algorithm &quot;Grows&quot; As a human, you might use intuition to decide which question to ask first. The computer, however, needs a metric. When the algorithm looks at your training data, it attempts to find the feature that best separates the target classes. Imagine a bucket containing 10 blue balls and 10 red balls. The bucket is &quot;impure&quot; (a 50/50 mix). The goal of the algorithm is to find a way to pour these balls into two new buckets such that the new buckets are as &quot;pure&quot; as possible (e.g., one bucket has mostly red, the other mostly blue). To do this, Scikit-Learn uses a metric called Gini Impurity (or sometimes Entropy).</p><ol><li>The model looks at every single feature (e.g., Age, Income, Debt). 2. It tests every possible split (e.g., Age &gt; 20, Age &gt; 21, Age &gt; 22...). 3. It calculates which split results in the highest &quot;purity&quot; (the most homogenous groups) in the resulting child nodes. 4. It repeats this process recursively for every child node until the leaves are pure or a stopping condition is met. A visual representation of splitting a 2D scatter plot. On the left, a plot with mixed red circles and blue squares. On the right, vertical and horizontal lines divide the plot into rectangular regions, isolating the red circles from the blue squares, illustrating how decision boundaries are created.</li></ol><p>A visual representation of splitting a 2D scatter plot. On the left, a plot with mixed red circles and blue squares. On the right, vertical and horizontal lines divide the plot into rectangular regions, isolating the red circles from the blue squares, illustrating how decision boundaries are created. Implementation in Scikit-Learn Let&#39;s apply this to a relatable scenario: Employee Retention. We want to predict if an employee will leave the company (Attrition = 1) or stay (Attrition = 0) based on their Satisfaction_Level (0 to 1) and Years_at_Company. We adhere to our modeling workflow: Instantiate, Fit, Predict.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.tree </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DecisionTreeClassifier</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.metrics </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> accuracy_score</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Setup Dummy Data</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Satisfaction_Level&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.85</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.15</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Years_at_Company&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:   [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;Attrition&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:          [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 0 = Stay, 1 = Leave</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame(data)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Define Features (X) and Target (y)</span></span></code></pre></div><p>X = df[[&#39;Satisfaction_Level&#39;, &#39;Years_at_Company&#39;]] y = df[&#39;Attrition&#39;]</p><h1 id="_3-instantiate-the-model" tabindex="-1">3. Instantiate the model <a class="header-anchor" href="#_3-instantiate-the-model" aria-label="Permalink to &quot;3. Instantiate the model&quot;">​</a></h1><h1 id="we-set-max-depth-to-prevent-the-tree-from-becoming-too-complex-more-on-this-later" tabindex="-1">We set max_depth to prevent the tree from becoming too complex (more on this later) <a class="header-anchor" href="#we-set-max-depth-to-prevent-the-tree-from-becoming-too-complex-more-on-this-later" aria-label="Permalink to &quot;We set max_depth to prevent the tree from becoming too complex (more on this later)&quot;">​</a></h1><p>tree_model = DecisionTreeClassifier(random_state=42, max_depth=3)</p><h1 id="_4-fit-the-model" tabindex="-1">4. Fit the model <a class="header-anchor" href="#_4-fit-the-model" aria-label="Permalink to &quot;4. Fit the model&quot;">​</a></h1><p>tree_model.fit(X, y)</p><h1 id="_5-predict" tabindex="-1">5. Predict <a class="header-anchor" href="#_5-predict" aria-label="Permalink to &quot;5. Predict&quot;">​</a></h1><h1 id="let-s-predict-for-a-new-employee-with-low-satisfaction-0-15-and-3-years-experience" tabindex="-1">Let&#39;s predict for a new employee with Low Satisfaction (0.15) and 3 Years experience <a class="header-anchor" href="#let-s-predict-for-a-new-employee-with-low-satisfaction-0-15-and-3-years-experience" aria-label="Permalink to &quot;Let&#39;s predict for a new employee with Low Satisfaction (0.15) and 3 Years experience&quot;">​</a></h1><p>new_employee = [[0.15, 3]] prediction = tree_model.predict(new_employee)</p><p>print(f&quot;Prediction (0=Stay, 1=Leave): {prediction[0]}&quot;) Visualizing the Logic One of the massive advantages of Decision Trees is that we can visualize the model logic directly without needing complex statistical interpretations. Scikit-Learn provides a tool to draw the tree we just built.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.tree </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plot_tree</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plot_tree(tree_model, </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">          feature_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Satisfaction&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Years&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],  </span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">          class_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Stay&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Leave&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">          filled</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> An output of the plot_tree function. The root node at the top shows a split condition </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Satisfaction &lt;= 0.5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The nodes are colored, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> shades of orange representing the </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Leave&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> class</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> shades of blue representing the </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Stay&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> class</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The color intensity indicates the purity of the node.</span></span></code></pre></div><p>An output of the plot_tree function. The root node at the top shows a split condition &#39;Satisfaction &lt;= 0.5&#39;. The nodes are colored, with shades of orange representing the &#39;Leave&#39; class and shades of blue representing the &#39;Stay&#39; class. The color intensity indicates the purity of the node. When you run this code, you will see exactly how the machine is thinking. It likely noticed that Satisfaction_Level was the most important predictor and placed it at the top (the root). If satisfaction is low, it predicts attrition; if high, it predicts retention. The Danger Zone: Overfitting You might be thinking, &quot;If I let the tree grow forever, won&#39;t it eventually classify every single training point correctly?&quot; Yes, and that is a problem. If you don&#39;t limit the growth of the tree, the algorithm will create specific rules for outliers. It essentially memorizes the training data rather than learning the general patterns. This is called Overfitting. An overfitted tree might look at a specific employee and create a rule: &quot;If Satisfaction is 0.612 AND Years is 4 AND Last Name starts with Z, then Leave.&quot; This works for the history books (training data), but it will fail miserably on new data (testing data) because that rule is just noise, not a trend. To prevent this, we use Hyperparameter Tuning. The most common controls are: <code>max_depth</code>: Limits how deep the tree can grow (e.g., only ask 3 questions). min_samples_split: Requires a certain amount of data in a node before allowed to split again (e.g., don&#39;t create a rule for just 2 people). Summary Decision Trees offer a refreshing change from the algebraic equations of Regression. They map logic in a way that mirrors human thought, making them exceptionally easy to explain to stakeholders. Pros: Interpretability: Easy to explain to non-technical audiences. Non-Linearity: Can capture complex, non-linear patterns (like &quot;If income is high OR income is low, but not medium...&quot;). Minimal Prep:* Requires less data cleaning (e.g., no need to scale/normalize features) compared to Regression. Cons: Overfitting: Without constraints (<code>max_depth</code>), they memorize noise. Instability: A small change in the data can result in a completely different tree structure. Because single Decision Trees are prone to overfitting and instability, data scientists rarely rely on just one tree for critical production models. Instead, they grow hundreds of trees and average their predictions. This leads us to the concept of Ensemble Modeling and the famous Random Forest, which we will discuss in the next chapter. Evaluating Model Performance: Confusion Matrix, Precision, and Recall In the previous sections, we added powerful tools to your arsenal: Logistic Regression and Decision Trees. You now possess the ability to predict binary outcomes—whether a customer will churn, whether a loan will default, or whether a transaction is fraudulent. However, simply building a model is not enough. In a business setting, you must answer the inevitable stakeholder question: &quot;How good is this model, really?&quot; Your instinct might be to answer with Accuracy—the percentage of correct predictions. While intuitive, accuracy can be the most dangerous metric in Data Science. To understand why, let’s imagine you are building a fraud detection system for a bank. The Accuracy Paradox Suppose you analyze a dataset of 1,000 credit card transactions. In reality, 990 are legitimate, and only 10 are fraudulent. If you wrote a &quot;dumb&quot; model that simply predicted &quot;Legitimate&quot; for every single transaction—ignoring the data entirely—your model would be correct 990 times out of 1,000. Your model would have 99% Accuracy. On paper, this looks spectacular. In practice, the model is useless. It failed to catch a single instance of fraud. This highlights the limitation of Accuracy: in datasets with imbalanced classes (where one outcome is much rarer than the other), accuracy hides the model&#39;s failures. To evaluate a classification model effectively, we need to look under the hood. We need the Confusion Matrix. The Confusion Matrix The Confusion Matrix is not a complex mathematical formula; it is a simple 2x2 tally sheet. It breaks down your model’s predictions into four distinct categories based on two questions: 1. What did the model predict? 2. What actually happened? A 2x2 grid representing a Confusion Matrix. The columns are labeled &#39;Predicted: No&#39; and &#39;Predicted: Yes&#39;. The rows are labeled &#39;Actual: No&#39; and &#39;Actual: Yes&#39;. The four quadrants are labeled: Top-Left &#39;True Negative (TN)&#39;, Top-Right &#39;False Positive (FP)&#39;, Bottom-Left &#39;False Negative (FN)&#39;, and Bottom-Right &#39;True Positive (TP)&#39;.</p><p>A 2x2 grid representing a Confusion Matrix. The columns are labeled &#39;Predicted: No&#39; and &#39;Predicted: Yes&#39;. The rows are labeled &#39;Actual: No&#39; and &#39;Actual: Yes&#39;. The four quadrants are labeled: Top-Left &#39;True Negative (TN)&#39;, Top-Right &#39;False Positive (FP)&#39;, Bottom-Left &#39;False Negative (FN)&#39;, and Bottom-Right &#39;True Positive (TP)&#39;. Let’s break down these four quadrants using a Customer Churn context (predicting if a customer will cancel their subscription).</p><ol><li>True Positive (TP): The model predicted the customer would churn, and they did. (A &quot;Hit&quot;). 2. True Negative (TN): The model predicted the customer would stay, and they did. (A correct non-event). 3. False Positive (FP): The model predicted the customer would churn, but they stayed. This is often called a Type I Error or a &quot;False Alarm.&quot; 4. False Negative (FN): The model predicted the customer would stay, but they churned. This is a Type II Error or a &quot;Miss.&quot; In a business context, not all errors are created equal. A False Positive might mean you send a discount coupon to a happy customer (a small cost). A False Negative might mean you lose a high-value client because you didn&#39;t know they were unhappy (a high cost). Precision and Recall Once we have the counts from the Confusion Matrix, we can calculate two specific metrics that tell us much more than simple accuracy: Precision and Recall. Precision: The &quot;Boy Who Cried Wolf&quot; Metric Precision answers the question: Of all the times the model predicted &#39;Yes&#39;, how often was it right? If your model predicts that 100 customers will churn, but only 20 actually do, your model has low precision. You are &quot;crying wolf&quot; too often. Low precision in a spam filter means legitimate emails are getting thrown into the junk folder (False Positives). $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$ Recall: The &quot;Fishing Net&quot; Metric Recall (also known as Sensitivity) answers the question: Of all the actual &#39;Yes&#39; cases in the data, how many did the model manage to find? If 100 customers actually churned, and your model successfully identified 90 of them, you have high recall. You cast a wide net. High recall is critical in medical diagnostics; if a patient has a disease, we cannot afford to miss it (False Negatives). $$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$ A diagram illustrating the difference between Precision and Recall. On the left, a &#39;Precision&#39; focus shows a small circle selecting only a few high-confidence red dots (positives) among blue dots (negatives), minimizing false positives. On the right, a &#39;Recall&#39; focus shows a large circle capturing all red dots but accidentally including several blue dots (false positives).</li></ol><p>A diagram illustrating the difference between Precision and Recall. On the left, a &#39;Precision&#39; focus shows a small circle selecting only a few high-confidence red dots (positives) among blue dots (negatives), minimizing false positives. On the right, a &#39;Recall&#39; focus shows a large circle capturing all red dots but accidentally including several blue dots (false positives). The Tug-of-War There is almost always a trade-off. If you tune your model to catch every fraudster (High Recall), you will inevitably flag some innocent customers (Lower Precision). If you tune your model to never falsely accuse an innocent customer (High Precision), you will inevitably miss some sophisticated fraudsters (Lower Recall). As a Data Scientist, your job is to ask the business stakeholder: Which error is more expensive? Is it worse to miss a sale (Low Recall)? Or is it worse to annoy a customer with irrelevant ads (Low Precision)? Implementing in Python Let&#39;s see how Scikit-Learn handles these metrics. We will assume we have already trained a Logistic Regression model named log_reg and have split our data into X_test and y_test.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.metrics </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> confusion_matrix, classification_report</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seaborn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sns</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Generate Predictions</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># The model predicts 0 (No Churn) or 1 (Churn)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_pred </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> log_reg.predict(X_test)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. Create the Confusion Matrix</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> confusion_matrix(y_test, y_pred)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. Visualize the Matrix</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sns.heatmap(cm, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">annot</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">fmt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;d&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cmap</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Blues&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cbar</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Predicted Label&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Actual Label&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Confusion Matrix for Customer Churn&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span></code></pre></div><p>The code above generates a heatmap allowing you to visually inspect the True Positives versus the errors. However, calculating the math manually is tedious. Scikit-Learn provides a summary tool called the Classification Report.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 4. Generate a full performance report</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(classification_report(y_test, y_pred))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Output Example:</span></span></code></pre></div><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>precision    recall  f1-score   support</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>           0       0.85      0.90      0.87       150</span></span>
<span class="line"><span>           1       0.75      0.60      0.67        50</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>    accuracy                           0.82       200</span></span>
<span class="line"><span>   macro avg       0.80      0.75      0.77       200</span></span>
<span class="line"><span>weighted avg       0.82      0.82      0.82       200</span></span></code></pre></div><p>Interpreting the Report: Class 1 (Churn): Precision (0.75): When the model predicts a customer will churn, it is correct 75% of the time. Recall (0.60): The model only caught 60% of the customers who actually churned. It missed 40% of them. F1-Score: This is the &quot;Harmonic Mean&quot; of Precision and Recall. It provides a single score that balances both metrics. If you need a balance between precision and recall, the F1-score is your go-to metric. Summary In this section, we moved beyond the &quot;Accuracy Trap.&quot; You learned that in real-world business problems—especially those involving rare events like fraud or churn—accuracy is often misleading. By using the Confusion Matrix, we can dissect exactly how our model is making mistakes. Use Precision when the cost of a False Positive is high (e.g., spam filters, stock market buy signals). Use Recall when the cost of a False Negative is high (e.g., disease screening, safety defects). Now that we can accurately evaluate our models, we are ready to explore how to improve them. In the next chapter, we will look at Ensemble Methods, where we combine multiple models (like Random Forests) to achieve performance that a single Decision Tree could never match. Case Study: Predicting Employee Attrition We have reached the synthesis of our classification journey. In the previous sections, we mastered the algorithms (Logistic Regression and Decision Trees) and learned the language of critique (Precision, Recall, and the Confusion Matrix). Now, we leave the classroom and enter the boardroom. We are going to apply these techniques to a domain that was historically dominated by &quot;gut feeling&quot; but is rapidly becoming one of the most data-intensive functions in business: Human Resources (HR) or &quot;People Analytics.&quot; In this case study, we will simulate a real-world project. You have been tasked by the Chief Human Resources Officer (CHRO) to solve a critical problem: Employee Attrition. The Business Problem Hiring new employees is expensive. Research suggests that the cost of replacing an employee ranges from 50% to 200% of their annual salary. Beyond the financial cost, high turnover lowers morale and results in lost institutional knowledge. The CHRO poses a challenge: &quot;We know people leave, but we don&#39;t know who, and we don&#39;t know why. Can you build a model to predict which employees are at risk of leaving so we can intervene before they resign?&quot; This is a classic Binary Classification problem. Input (X): Employee demographics, job role, satisfaction scores, overtime history, etc. Output (y): Attrition (Yes/No). Step 1: Data Preparation and Encoding For this case study, we will use a dataset commonly referenced in the industry (based on IBM HR Analytics data). It contains numerical data (like Age) and categorical data (like Department). As we discussed in the Feature Engineering chapter, machine learning models (mostly) speak math, not English. They cannot understand the string &quot;Sales&quot; or &quot;Research.&quot; We must translate these categories into numbers. We will use a technique called One-Hot Encoding (or dummy variables). This process creates a new binary column for every unique category. A conceptual diagram of One-Hot Encoding. On the left, a single column named &quot;Department&quot; contains values &quot;Sales&quot;, &quot;R&amp;D&quot;, and &quot;HR&quot;. An arrow points to the right showing three new columns: &quot;Department_Sales&quot;, &quot;Department_RnD&quot;, and &quot;Department_HR&quot;. The rows contain 1s and 0s indicating membership to those categories.</p><p>A conceptual diagram of One-Hot Encoding. On the left, a single column named &quot;Department&quot; contains values &quot;Sales&quot;, &quot;R&amp;D&quot;, and &quot;HR&quot;. An arrow points to the right showing three new columns: &quot;Department_Sales&quot;, &quot;Department_RnD&quot;, and &quot;Department_HR&quot;. The rows contain 1s and 0s indicating membership to those categories. Let&#39;s look at the Python implementation using pandas:</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.model_selection </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_test_split</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Load the dataset (hypothetical path)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.read_csv(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;hr_employee_attrition.csv&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. Select relevant features</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We drop &#39;EmployeeCount&#39; and &#39;StandardHours&#39; as they are the same for everyone</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Age&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Department&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;DistanceFromHome&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;EnvironmentSatisfaction&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &#39;OverTime&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;MonthlyIncome&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;JobRole&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;Attrition&#39;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # Values are &#39;Yes&#39; or &#39;No&#39;</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[features]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> df[target].apply(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;Yes&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> else</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Convert Target to 1/0</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. One-Hot Encoding for categorical variables (Department, OverTime, JobRole)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># drop_first=True avoids multicollinearity (redundancy)</span></span></code></pre></div><p>X_encoded = pd.get_dummies(X, drop_first=True)</p><p>print(&quot;Original shape:&quot;, X.shape) print(&quot;Encoded shape:&quot;, X_encoded.shape) When you run this, you will notice the number of columns increases. The column OverTime (containing &quot;Yes&quot;/&quot;No&quot;) becomes OverTime_Yes (containing 1/0). This prepares our data for the algorithm. Step 2: Training the Model For this problem, we will choose a Decision Tree Classifier. Why? Because in HR, explainability is paramount. If our model flags an employee as &quot;High Risk,&quot; the HR manager will immediately ask, &quot;Why?&quot; A Decision Tree provides clear logic (e.g., &quot;Because they work Overtime AND live far away&quot;) that a Neural Network or a complex ensemble might hide.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.tree </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DecisionTreeClassifier</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Split data: 80% for training, 20% for testing</span></span></code></pre></div><p>X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)</p><h1 id="initialize-the-decision-tree" tabindex="-1">Initialize the Decision Tree <a class="header-anchor" href="#initialize-the-decision-tree" aria-label="Permalink to &quot;Initialize the Decision Tree&quot;">​</a></h1><h1 id="we-limit-depth-to-avoid-overfitting-making-the-tree-too-complex" tabindex="-1">We limit depth to avoid overfitting (making the tree too complex) <a class="header-anchor" href="#we-limit-depth-to-avoid-overfitting-making-the-tree-too-complex" aria-label="Permalink to &quot;We limit depth to avoid overfitting (making the tree too complex)&quot;">​</a></h1><p>clf = DecisionTreeClassifier(max_depth=4, random_state=42)</p><h1 id="train-the-model" tabindex="-1">Train the model <a class="header-anchor" href="#train-the-model" aria-label="Permalink to &quot;Train the model&quot;">​</a></h1><p>clf.fit(X_train, y_train)</p><p>print(&quot;Model training complete.&quot;) Step 3: Evaluation and Business Logic Now comes the critical step: answering the stakeholder&#39;s question, &quot;How good is the model?&quot; In the previous section, we learned about the Confusion Matrix. Let&#39;s apply it here. In the context of Employee Attrition:</p><ul><li>True Positive (TP): We predicted they would leave, and they did. (Success: We might have saved them).</li><li>True Negative (TN): We predicted they would stay, and they stayed. (Success).</li><li>False Positive (FP): We predicted they would leave, but they stayed. (The &quot;Crying Wolf&quot; error).</li><li>False Negative (FN): We predicted they would stay, but they left. (The &quot;Missed Opportunity&quot; error).</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.metrics </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> confusion_matrix, classification_report</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Make predictions on the unseen test set</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_pred </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clf.predict(X_test)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Generate the metrics</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(classification_report(y_test, y_pred))</span></span></code></pre></div><p>Interpreting the Results for Stakeholders: Let&#39;s assume your model produces the following Recall score for the &quot;Leavers&quot; class (Class 1): 0.45 (45%). If you present this simply as a number, you might fail to convey the value. You must translate this into business terms: &quot;Current status: We currently react to resignations after they happen. &gt; &gt; New Model status: This model effectively identifies 45% of the employees who are about to resign before they turn in their letter. While it misses some (False Negatives), it gives HR a targeted list of at-risk employees to engage with, rather than guessing blindly.&quot; Step 4: Visualizing the Decision Logic The true power of the Decision Tree is visual. We can plot the tree to understand the root causes of attrition.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.tree </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plot_tree</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.figure(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">figsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plot_tree(clf, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">feature_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_encoded.columns, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">class_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Stay&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Leave&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">filled</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">fontsize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A visualization of a Decision Tree </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> HR</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> data. The Root Node at the top shows </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;OverTime_Yes &lt;= 0.5&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The branch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;True&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (No Overtime) goes left to a blue node indicating most people stay. The branch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;False&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (Yes Overtime) goes right to a node asking </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;MonthlyIncome &lt;= 3000&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">. The visualization demonstrates how the model splits employees into risk pools based on specific criteria.</span></span></code></pre></div><p>A visualization of a Decision Tree for HR data. The Root Node at the top shows &quot;OverTime_Yes &lt;= 0.5&quot;. The branch for &quot;True&quot; (No Overtime) goes left to a blue node indicating most people stay. The branch for &quot;False&quot; (Yes Overtime) goes right to a node asking &quot;MonthlyIncome &lt;= 3000&quot;. The visualization demonstrates how the model splits employees into risk pools based on specific criteria. By analyzing this tree, you might discover insights to feed back to management: 1. The &quot;Overtime&quot; Split: The very first split often separates those who work overtime from those who don&#39;t. This suggests burnout is a primary driver. 2. The &quot;Income&quot; Split: Among those working overtime, low income creates a high-risk &quot;leaf node.&quot; Step 5: Feature Importance Finally, we can extract the &quot;Feature Importance&quot; scores. This tells us which variables had the heaviest weight in the decision-making process.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pandas </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Create a dataframe of feature importance</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">importance </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pd.DataFrame({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Feature&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: X_encoded.columns, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Importance&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: clf.feature_importances_})</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(importance.sort_values(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">by</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Importance&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">ascending</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).head(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p>Sample Output: 1. OverTime_Yes: 0.28 2. MonthlyIncome: 0.21 3. Age: 0.15 4. DistanceFromHome: 0.10 5. JobRole_SalesRepresentative: 0.08 Conclusion: From Prediction to Policy This case study illustrates the transition from &quot;Data Science&quot; to &quot;Business Intelligence.&quot; You started with raw data and ended with a strategic recommendation: To reduce turnover, the company should review its Overtime policies, specifically for lower-income Sales Representatives who live far from the office. We did not just predict who would leave; we used the transparency of the Decision Tree to understand what needs to change in the organization. In the next chapter, we will move away from Supervised Learning (where we have the answers) and explore Unsupervised Learning, where we ask the machine to discover hidden patterns in data without any guidance at all.</p>`,70)])])}const g=i(n,[["render",l]]);export{c as __pageData,g as default};
